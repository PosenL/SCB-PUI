{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from itertools import chain\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current file location: c:\\Planning S&P\\Project\\SCB_PUI\n",
      "Login WBI : nxf83451\n"
     ]
    }
   ],
   "source": [
    "Program_Start_Time  = time.time()\n",
    "print(\"Current file location: \"+ os.getcwd())\n",
    "print(\"Login WBI : \"+ os.getlogin())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Input Reference (PUI and JDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loading ....\n"
     ]
    }
   ],
   "source": [
    "print(\"File loading ....\")\n",
    "\n",
    "df_PUI_raw = pd.read_excel(r\"./Input/Supply_Chain_PUI.xlsx\", sheet_name=\"Raw\")\n",
    "df_JDA_input = pd.read_excel(r\"./Input/Input_JDA_List.xlsx\")\n",
    "\n",
    "df_PUI_raw[\"EFF_FR_DATE\"] = df_PUI_raw[\"EFF_FR_DATE\"].astype(str).str.split(\" \", expand=True)[0]\n",
    "df_PUI_raw[\"EFF_TO_DATE\"] = df_PUI_raw[\"EFF_TO_DATE\"].astype(str).str.split(\" \", expand=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PUI_raw[\"COMP_12NC\"] = df_PUI_raw[\"1_12NC_LIST\"].copy()\n",
    "df_PUI_raw[\"SingleComp_index\"] = df_PUI_raw.index\n",
    "df_PUI_raw.loc[(df_PUI_raw[\"COMP_12NC_LIST\"].str.contains(\",\")) & (df_PUI_raw[\"TYPE\"] == \"ICAM\"), \"COMP_12NC\"] = df_PUI_raw.loc[(df_PUI_raw[\"COMP_12NC_LIST\"].str.contains(\",\")) & (df_PUI_raw[\"TYPE\"] == \"ICAM\"), \"COMP_12NC_LIST\"]\n",
    "df_PUI_raw[\"COMP_12NC\"] = df_PUI_raw[\"COMP_12NC\"].astype(str)\n",
    "\n",
    "#Get component 12NC and extend them if they are mutlicomponent\n",
    "MutiComp_stack = pd.DataFrame([])\n",
    "temp = df_PUI_raw.loc[df_PUI_raw[\"COMP_12NC\"].str.contains(\",\")]\n",
    "temp.loc[temp[\"COMP_12NC\"] .str.contains(\",\"), \"MutiComp\"] = temp.loc[temp[\"COMP_12NC\"].str.contains(\",\"), \"COMP_12NC\"]\n",
    "\n",
    "\n",
    "temp[\"SingleComp_index\"] = pd.NaT\n",
    "temp.loc[temp[\"COMP_12NC\"].str.contains(\",\"), \"MutiComp_index\"] = temp.loc[temp[\"COMP_12NC\"].str.contains(\",\")].index\n",
    "temp[\"MutiComp_index\"] = temp[\"MutiComp_index\"].astype(int)\n",
    "\n",
    "def split_rows(df, column):\n",
    "    # Create an empty DataFrame to store the new rows\n",
    "    new_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Check if the value in the specific column contains a comma\n",
    "        if \",\" in str(row[column]):\n",
    "            # Split the value by comma\n",
    "            split_values = str(row[column]).split(',')\n",
    "            for value in split_values:\n",
    "                # Create a new row with the split value\n",
    "                new_row = row.copy()\n",
    "                new_row[column] = value.strip()  # Remove any leading/trailing whitespace\n",
    "                new_df = new_df.append(new_row, ignore_index=True)\n",
    "        else:\n",
    "            # If no comma, just append the row as is\n",
    "            new_df = new_df.append(row, ignore_index=True)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "df_extend_multicomp = split_rows(temp, 'COMP_12NC')\n",
    "df_extend_multicomp[\"COMP_12NC\"] = df_extend_multicomp[\"COMP_12NC\"].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the original df: 59929\n",
      "Length of the extended Multi-Component df: 1048\n",
      "Length of the Aggregated df: 60977, True\n",
      "Length of the Aggreaged df and drop multiple list: 60462\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 60462 entries, 0 to 60976\n",
      "Data columns (total 33 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   HEADING           60462 non-null  object \n",
      " 1   ACTION            0 non-null      float64\n",
      " 2   PPF               25225 non-null  object \n",
      " 3   TG5               56404 non-null  object \n",
      " 4   PART_12NC         60462 non-null  int64  \n",
      " 5   TYPE              60462 non-null  object \n",
      " 6   LEGACY_PART       271 non-null    object \n",
      " 7   LOC               60462 non-null  object \n",
      " 8   PLANT             60462 non-null  object \n",
      " 9   OPTIONS           60462 non-null  object \n",
      " 10  STATUS            60462 non-null  object \n",
      " 11  EFF_FR_DATE       60462 non-null  object \n",
      " 12  EFF_TO_DATE       60462 non-null  object \n",
      " 13  COMP_PART_LIST    265 non-null    object \n",
      " 14  COMP_12NC_LIST    60462 non-null  object \n",
      " 15  PBOM_ID           60462 non-null  object \n",
      " 16  LEN               60462 non-null  object \n",
      " 17  1_12NC_LIST       60462 non-null  object \n",
      " 18  1_CLASS           60462 non-null  object \n",
      " 19  2_12NC_LIST       13989 non-null  float64\n",
      " 20  2_CLASS           13989 non-null  object \n",
      " 21  3_12NC_LIST       60 non-null     float64\n",
      " 22  3_CLASS           60 non-null     object \n",
      " 23  4_12NC_LIST       16 non-null     float64\n",
      " 24  4_CLASS           16 non-null     object \n",
      " 25  Unnamed: 25       13856 non-null  float64\n",
      " 26  Unnamed: 26       46 non-null     float64\n",
      " 27  Unnamed: 27       16 non-null     float64\n",
      " 28  Unnamed: 28       16 non-null     float64\n",
      " 29  COMP_12NC         60462 non-null  int64  \n",
      " 30  SingleComp_index  59414 non-null  object \n",
      " 31  MutiComp          1048 non-null   object \n",
      " 32  MutiComp_index    1048 non-null   object \n",
      "dtypes: float64(8), int64(2), object(23)\n",
      "memory usage: 15.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_agg = pd.concat([df_PUI_raw, df_extend_multicomp])\n",
    "df_agg.reset_index(drop = True, inplace =True)\n",
    "\n",
    "print(\"Length of the original df: {}\".format(len(df_PUI_raw)))\n",
    "print(\"Length of the extended Multi-Component df: {}\".format(len(df_extend_multicomp)))\n",
    "print(\"Length of the Aggregated df: {}, {}\".format(len(df_agg), len(df_agg) == (len(df_PUI_raw) + len(df_extend_multicomp))))\n",
    "\n",
    "df_agg_final = df_agg[~df_agg.index.isin(df_agg[\"MutiComp_index\"].dropna())]\n",
    "\n",
    "print(\"Length of the Aggreaged df and drop multiple list: {}\\n\".format(len(df_agg_final)))\n",
    "\n",
    "\n",
    "# Dict for adjacent 12NC\n",
    "df_agg_final[\"PART_12NC\"] = df_agg_final[\"PART_12NC\"].astype(\"int64\")\n",
    "df_agg_final[\"COMP_12NC\"] = df_agg_final[\"COMP_12NC\"].astype(\"int64\")\n",
    "print(df_agg_final.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49229\n"
     ]
    }
   ],
   "source": [
    "# Dict for adjacent 12NC\n",
    "prod = {}\n",
    "for x in df_agg_final[\"PART_12NC\"].unique():\n",
    "    prod[x] = df_agg_final.loc[df_agg_final['PART_12NC'] == x, \"COMP_12NC\"].to_list()\n",
    "print(len(prod))\n",
    "\n",
    "# Dict for Matertial Type\n",
    "material_12nc_type_list = [[\"PART_12NC\", \"TYPE\"],\n",
    "                           [\"1_12NC_LIST\", \"1_CLASS\"], \n",
    "                           [\"2_12NC_LIST\", \"2_CLASS\"], \n",
    "                           [\"3_12NC_LIST\", \"3_CLASS\"], \n",
    "                           [\"4_12NC_LIST\", \"4_CLASS\"]]\n",
    "\n",
    "\n",
    "def Merge(dict1, dict2):\n",
    "    res = {**dict1, **dict2}\n",
    "    return res\n",
    "\n",
    "Mtype = dict()\n",
    "for material_12nc_type in (material_12nc_type_list):\n",
    "    sub_material = df_agg[material_12nc_type[0]].dropna()\n",
    "    sub_type = df_agg[material_12nc_type[1]].dropna()\n",
    "    sub_Mtype = dict(zip(sub_material,sub_type))\n",
    "    Mtype = Merge(Mtype, sub_Mtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PUI_Structure_Rolling Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PUI_Structure_Rolling:\n",
    "    def __init__(self):\n",
    "        self.SC_Num = 0\n",
    "        self.PUI_anlyzied_slice_stack = pd.DataFrame([])\n",
    "        self.df_all_valid_supply_path = pd.DataFrame([])\n",
    "        self.PUI_stack = pd.DataFrame([])\n",
    "        self.PUI_all_stack = pd.DataFrame([])\n",
    "\n",
    "    def main(self, input_SC_dataframe, input_PUI_dataframe, input_PUI_ref_before_drop_multi_comp):\n",
    "        self.input_SC_List_len = len(input_SC_dataframe)\n",
    "\n",
    "        print(\"Rolling Supply Path ===========================================================\")\n",
    "        for prod_item, consumed_item, consumed_plant, BOM_id in tqdm(zip(input_SC_dataframe[\"ITEM_NAME\"], input_SC_dataframe[\"CONSUMED_ITEM\"], input_SC_dataframe[\"PLANT\"], input_SC_dataframe[\"ITEM_BOM_RT_ID\"]), total = len(input_SC_dataframe)):\n",
    "            self.SC_Num += 1\n",
    "            all_paths_list = self.find_unique_path(prod, prod_item, consumed_item) # Get all applicable paths (prod to comp) -> list\n",
    "            if len(all_paths_list) == 0:\n",
    "                continue\n",
    "            self.df_all_valid_supply_path = pd.concat([self.df_all_valid_supply_path, self.Mtype_Aggregation(all_paths_list)]) # Get all applicable supply chain with Mtype Mapping -> dataframe\n",
    "            df_SC_analyezd_Stack = self.SC_Structure_Extraction(input_PUI_dataframe, all_paths_list, consumed_plant, BOM_id) # Extract each supply path and check if the supply path is valid\n",
    "            \n",
    "            #Result concating for each input supply path combination\n",
    "            df_SC_analyezd_Stack[\"SC Index\"] = self.SC_Num\n",
    "            self.PUI_anlyzied_slice_stack = pd.concat([self.PUI_anlyzied_slice_stack, df_SC_analyezd_Stack]) \n",
    "\n",
    "        print(\"\\nMulti-Component Checking ======================================================\")\n",
    "        df_component_analyzed_result  = self.MultiComponent_Checking(self.PUI_anlyzied_slice_stack, input_SC_dataframe)\n",
    "        if len(df_component_analyzed_result) != 0:\n",
    "            df_component_analyzed_result.insert(0, 'SC Index', df_component_analyzed_result.pop('SC Index'))\n",
    "            df_component_analyzed_result.insert(2, 'BOM_ID', df_component_analyzed_result.pop('BOM_ID'))\n",
    "\n",
    "        print(\"\\nPUI Mapping Result ============================================================\")\n",
    "        df_PUI_mapping = self.PUI_result_mapping(input_PUI_ref_before_drop_multi_comp, df_component_analyzed_result)\n",
    "        \n",
    "        #Result proprocessing\n",
    "        if len(df_PUI_mapping) != 0:\n",
    "            df_PUI_mapping[\"PUI_Index\"] = df_PUI_mapping.index\n",
    "            df_PUI_mapping[\"PART_12NC\"] = df_PUI_mapping[\"PART_12NC\"].astype(str)\n",
    "            df_PUI_mapping[\"COMP_12NC\"] = df_PUI_mapping[\"COMP_12NC\"].astype(str)\n",
    "            df_PUI_mapping.insert(0, 'PUI_Index', df_PUI_mapping.pop('PUI_Index'))\n",
    "            df_PUI_mapping.insert(1, 'SC Index', df_PUI_mapping.pop('SC Index'))\n",
    "            df_PUI_mapping.insert(2, 'BOM_ID', df_PUI_mapping.pop('BOM_ID'))\n",
    "            df_PUI_mapping.insert(10, 'COMP_12NC', df_PUI_mapping.pop('COMP_12NC'))\n",
    "            df_PUI_mapping.drop(columns = [\"SingleComp_index\", \"MutiComp\", \"MutiComp_index\"], inplace = True)\n",
    "            df_PUI_mapping.reset_index(drop= True, inplace = True)\n",
    "\n",
    "        return df_PUI_mapping, df_component_analyzed_result, self.df_all_valid_supply_path\n",
    "\n",
    "    def find_all_paths(self, graph, start, end, path=None, unique_paths=None):\n",
    "        if path is None:\n",
    "            path = []\n",
    "        if unique_paths is None:\n",
    "            unique_paths = set()\n",
    "        path.append(start)\n",
    "        if start == end:\n",
    "            # Convert path to a tuple so it can be added to a set\n",
    "            unique_paths.add(tuple(path))\n",
    "        else:\n",
    "            for node in graph.get(start, []):\n",
    "                if node not in path:  # Avoid cycles\n",
    "                    self.find_all_paths(graph, node, end, path.copy(), unique_paths)\n",
    "        return unique_paths\n",
    "\n",
    "\n",
    "    def find_unique_path(self, graph, start, end, path=None, unique_paths=None):\n",
    "        # Find all paths\n",
    "        all_paths = self.find_all_paths(graph, start, end)\n",
    "        # Convert each tuple path back to a list if needed\n",
    "        all_paths = [list(path) for path in all_paths]\n",
    "        # print(\"============================================================\")\n",
    "        # print(\"Feasible Combinations: \")\n",
    "        # print(\"({}/{})\".format(self.SC_Num, self.input_SC_List_len) , all_paths)\n",
    "        return all_paths\n",
    "\n",
    "\n",
    "    def Mtype_Aggregation(self, input_all_paths):\n",
    "        Info = []\n",
    "        for x in input_all_paths:\n",
    "            Info_sub = []\n",
    "            for y in x:\n",
    "                Info_sub.append(Mtype[y])\n",
    "            Info.append(Info_sub)\n",
    "\n",
    "        #Output\n",
    "        stacker = []\n",
    "        for x, y in zip(Info, input_all_paths):\n",
    "            stacker.append(x)\n",
    "            stacker.append(y)\n",
    "        output_df = pd.DataFrame(stacker)\n",
    "        output_df.columns = [('Element_' + str(x + 1)) for x in range(len(output_df.columns))]\n",
    "        output_df[\"SC Index\"] = self.SC_Num\n",
    "        output_df.insert(0, 'SC Index', output_df.pop('SC Index'))\n",
    "        return output_df\n",
    "\n",
    "    def SC_Structure_Extraction(self, input_PUI_ref, PUI_valid_path_list, consumed_plant, BOM_id):\n",
    "        SC_Stack = []\n",
    "        idx = 0\n",
    "        \n",
    "        for x in PUI_valid_path_list:\n",
    "            idx +=1\n",
    "            for y in range(len(x)):\n",
    "                try:\n",
    "                    SC_Stack.append([idx, x[y], x[y+1], input_PUI_ref.loc[(input_PUI_ref[\"PART_12NC\"] == x[y]) & (input_PUI_ref[\"COMP_12NC\"] == x[y+1]), \"PLANT\"].unique()])\n",
    "                except:\n",
    "                    # SC_Stack.append([\"\", \"\", \"\", \"\"])\n",
    "                    continue\n",
    "\n",
    "            \n",
    "        df_SC_analyezd_Stack = pd.DataFrame(SC_Stack, columns = [\"Rolling Result - Combination\", \"Produced\", \"Consumed\", \"Site\"])\n",
    "        #Check if target plant is in Available Plant for each combination\n",
    "        df_SC_analyezd_Stack[\"Target Plant\"] = consumed_plant\n",
    "        df_SC_analyezd_Stack[\"BOM_ID\"] = BOM_id\n",
    "        df_SC_analyezd_Stack[\"Available Plant\"] = df_SC_analyezd_Stack.apply(lambda x: True if consumed_plant in x[\"Site\"] else False, axis = 1)\n",
    "\n",
    "\n",
    "        for c in df_SC_analyezd_Stack[\"Rolling Result - Combination\"].unique():\n",
    "            if c != '':\n",
    "                each_comb = df_SC_analyezd_Stack.loc[(df_SC_analyezd_Stack[\"Rolling Result - Combination\"] == c)]\n",
    "                if (each_comb[\"Available Plant\"] == False).any() == True:\n",
    "                    df_SC_analyezd_Stack.loc[(df_SC_analyezd_Stack[\"Rolling Result - Combination\"] == c), \"Valid_Plant\"] = False\n",
    "                else:\n",
    "                    df_SC_analyezd_Stack.loc[(df_SC_analyezd_Stack[\"Rolling Result - Combination\"] == c), \"Valid_Plant\"] = True\n",
    "\n",
    "        df_SC_analyezd_Stack.fillna(\"\", inplace = True)\n",
    "        return df_SC_analyezd_Stack\n",
    "\n",
    "    def MultiComponent_Checking(self, input_PUI_anlyzied_slice_stack, input_SC_dataframe):\n",
    "        if len(input_PUI_anlyzied_slice_stack) == 0:\n",
    "            return pd.DataFrame([])\n",
    "        else: \n",
    "            for bom_id in tqdm(input_PUI_anlyzied_slice_stack[\"BOM_ID\"].unique(), total = len(input_PUI_anlyzied_slice_stack[\"BOM_ID\"].unique())):\n",
    "                df_extract_from_bom_id = input_PUI_anlyzied_slice_stack.loc[(input_PUI_anlyzied_slice_stack[\"BOM_ID\"] == bom_id)]\n",
    "                if len(df_extract_from_bom_id[\"SC Index\"].unique()) != len(input_SC_dataframe.loc[input_SC_dataframe[\"ITEM_BOM_RT_ID\"] == bom_id]):\n",
    "                    input_PUI_anlyzied_slice_stack.loc[(input_PUI_anlyzied_slice_stack[\"BOM_ID\"] == bom_id), \"Valid_Comp_Combination\"] = False\n",
    "                    continue\n",
    "                else:\n",
    "                    for comb_id in df_extract_from_bom_id[\"Rolling Result - Combination\"].unique():\n",
    "                        df_extract_from_bom_id_and_comb_id = df_extract_from_bom_id[df_extract_from_bom_id[\"Rolling Result - Combination\"] == comb_id]\n",
    "                        if (df_extract_from_bom_id_and_comb_id[\"Valid_Plant\"] == False).any():\n",
    "                            input_PUI_anlyzied_slice_stack.loc[(input_PUI_anlyzied_slice_stack[\"BOM_ID\"] == bom_id) & (input_PUI_anlyzied_slice_stack[\"Rolling Result - Combination\"] == comb_id), \"Valid_Comp_Combination\"] = False\n",
    "                        else:\n",
    "                            input_PUI_anlyzied_slice_stack.loc[(input_PUI_anlyzied_slice_stack[\"BOM_ID\"] == bom_id) & (input_PUI_anlyzied_slice_stack[\"Rolling Result - Combination\"] == comb_id), \"Valid_Comp_Combination\"] = True\n",
    "            return input_PUI_anlyzied_slice_stack\n",
    "\n",
    "    def PUI_result_mapping(self, input_PUI_ref_before_drop_multi_comp, input_SC_analyezd_Stack):\n",
    "        if len(input_SC_analyezd_Stack) ==0:\n",
    "            return pd.DataFrame([])\n",
    "        else:\n",
    "            for idx, bom_id, p, c, plt, val in tqdm(zip(input_SC_analyezd_Stack[\"SC Index\"], input_SC_analyezd_Stack[\"BOM_ID\"], input_SC_analyezd_Stack[\"Produced\"], input_SC_analyezd_Stack[\"Consumed\"], input_SC_analyezd_Stack[\"Target Plant\"], input_SC_analyezd_Stack[\"Valid_Comp_Combination\"]), total = len(input_SC_analyezd_Stack[\"SC Index\"])):\n",
    "                if val == True:\n",
    "                    sub_PUI_index_single = input_PUI_ref_before_drop_multi_comp.loc[(input_PUI_ref_before_drop_multi_comp[\"PART_12NC\"] == p) & (input_PUI_ref_before_drop_multi_comp[\"COMP_12NC\"] == str(c)) & (input_PUI_ref_before_drop_multi_comp[\"PLANT\"] == plt), \"SingleComp_index\"].dropna().values\n",
    "                    sub_PUI_index_multi = input_PUI_ref_before_drop_multi_comp.loc[(input_PUI_ref_before_drop_multi_comp[\"PART_12NC\"] == p) & (input_PUI_ref_before_drop_multi_comp[\"COMP_12NC\"] == str(c)) & (input_PUI_ref_before_drop_multi_comp[\"PLANT\"] == plt), \"MutiComp_index\"].dropna().values\n",
    "                    sub_PUI_index = set(sub_PUI_index_single).union(set(sub_PUI_index_multi))\n",
    "\n",
    "                    sub_PUI = input_PUI_ref_before_drop_multi_comp.loc[sub_PUI_index]\n",
    "                    sub_PUI[\"SC Index\"] = idx\n",
    "                    sub_PUI[\"BOM_ID\"] = bom_id\n",
    "                    \n",
    "                    ###\n",
    "                    # print(\"\\r({}/{})\".format(idx, input_SC_analyezd_Stack[\"SC Index\"].max), bom_id, p, c, plt, val, \"sub_PUI_index: {}\".format(sub_PUI_index), end = \"\", flush = True)\n",
    "\n",
    "                    if len(sub_PUI_index) != 0:\n",
    "                        self.PUI_stack = pd.concat([self.PUI_stack, sub_PUI])\n",
    "\n",
    "            return self.PUI_stack\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Checking and Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuslt_PUI_anlyzied_slice.insert(0, 'SC Index', Reuslt_PUI_anlyzied_slice.pop('SC Index'))\n",
    "# Reuslt_PUI_anlyzied_slice\n",
    "\n",
    "# Result_SP.insert(0, 'SC Index', Result_SP.pop('SC Index'))\n",
    "# Result_SP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from styleframe import StyleFrame, Styler, utils\n",
    "\n",
    "class General_function:\n",
    "     def __init__(self):\n",
    "          self.today = (datetime.datetime.today()).strftime('%Y%m%d')\n",
    "          self.output_path = \"./Output/Result_table_{}.xlsx\".format(self.today)\n",
    "\n",
    "     def Export_to_excel_with_style(self, input_Result, input_Result_SP, input_Reuslt_PUI_anlyzied_slice):\n",
    "          Result_frame = General_function().style_changes(input_Result, \n",
    "                                                          [[\"PUI_Index\", \"SC Index\", \"BOM_ID\"], [\"PART_12NC\", \"TYPE\", \"COMP_12NC\"], [\"COMP_12NC_LIST\"], [\"PLANT\"]],\n",
    "                                                          [\"yellow\", \"#85C1E9\", \"#85C1E9\", \"green\"],\n",
    "                                                          )\n",
    "          Result_drop_dup_frame = General_function().style_changes(input_Result.drop(columns=[\"SC Index\", \"BOM_ID\"]).drop_duplicates().sort_values(by = \"PUI_Index\"), \n",
    "                                                          [[\"PUI_Index\"], [\"PART_12NC\", \"TYPE\", \"COMP_12NC\"], [\"COMP_12NC_LIST\"], [\"PLANT\"]],\n",
    "                                                          [\"yellow\", \"#85C1E9\", \"#85C1E9\", \"green\"],\n",
    "                                                          )\n",
    "          Result_SP_frame = General_function().style_changes(input_Result_SP.reset_index(drop = True), \n",
    "                                                          [[\"SC Index\", \"Rolling Result - Combination\"], [\"Produced\", \"Consumed\"], [\"Target Plant\"], [\"BOM_ID\"]],\n",
    "                                                          [\"yellow\", \"#85C1E9\", \"green\"],\n",
    "                                                          )\n",
    "\n",
    "          Reuslt_PUI_anlyzied_slice_frame = General_function().style_changes(input_Reuslt_PUI_anlyzied_slice.reset_index(drop = True),\n",
    "                                                          [[\"SC Index\"]],\n",
    "                                                          [\"yellow\"],\n",
    "                                                          )\n",
    "          #Export to excel\n",
    "          excel_writer = StyleFrame.ExcelWriter(self.output_path)\n",
    "          Result_frame.to_excel(excel_writer, sheet_name = \"Result - PUI Mapping\", index= False, header = True)\n",
    "          Result_drop_dup_frame.to_excel(excel_writer, sheet_name = \"Result - PUI Mapping (Drop_dup)\", index= False, header = True)\n",
    "          Result_SP_frame.to_excel(excel_writer, sheet_name = \"Analyzed - Rolling Supply Path\", index= False, header = True)\n",
    "          Reuslt_PUI_anlyzied_slice_frame.to_excel(excel_writer, sheet_name = \"Raw - Rolling Supply Chain\", index= False, header = True)\n",
    "          excel_writer.save()\n",
    "\n",
    "          return \"Data exported completedly !\"\n",
    "          \n",
    "     def style_changes(self, input_frame, update_columns_combination, color_combination):\n",
    "          #Summary Table\n",
    "          if len(input_frame) == 0:\n",
    "               return StyleFrame(pd.DataFrame([]))\n",
    "          else:\n",
    "               sf_output_ref = StyleFrame(input_frame)\n",
    "               for h_col in input_frame.columns:\n",
    "                    sf_output_ref[h_col] = input_frame[h_col].fillna(\"\").values\n",
    "                    if h_col not in [\"EFF_FR_DATE\", \"EFF_TO_DATE\"]:\n",
    "                         sf_output_ref[h_col] = sf_output_ref[h_col].astype(str)\n",
    "                    header_width = input_frame[h_col].astype(str).str.len().max() + 5\n",
    "                    if header_width <= len(h_col):\n",
    "                         header_width = len(h_col) + 5\n",
    "                    sf_output_ref.apply_column_style(cols_to_style = h_col,\n",
    "                                                  styler_obj=Styler(font=utils.fonts.calibri, font_size= 11),\n",
    "                                                  width=header_width,\n",
    "                                                  style_header=False\n",
    "                                                  )\n",
    "\n",
    "                    \n",
    "               sf_output_ref.apply_headers_style(styler_obj = Styler(font = 'Calibri', font_size = 11, bg_color = \"#FDEBD0\"), cols_to_style = input_frame.columns)\n",
    "               for col_list, color in zip(update_columns_combination, color_combination):\n",
    "                    sf_output_ref.apply_headers_style(styler_obj = Styler(font = 'Calibri', font_size = 11, bg_color = color), cols_to_style = col_list)\n",
    "\n",
    "               return sf_output_ref     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in 24.46 mins.\n",
      "Rolling Supply Path ===========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 122.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multi-Component Checking ======================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 450.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PUI Mapping Result ============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:01<00:00, 80.06it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Result is now exporting to excel files ...... \n",
      ">> Export Process is now completed !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__=='__main__':\n",
    "    print(\"Data loaded in {} mins.\".format(round((time.time() - Program_Start_Time)/60, 2)))\n",
    "    Result, Result_SP, Reuslt_PUI_anlyzied_slice = PUI_Structure_Rolling().main(df_JDA_input, df_agg_final, df_agg)\n",
    "\n",
    "    # Export to excel with style\n",
    "    print(\"\\n>> Result is now exporting to excel files ...... \")\n",
    "    General_function().Export_to_excel_with_style(Result, Result_SP, Reuslt_PUI_anlyzied_slice)\n",
    "    print(\">> Export Process is now completed !\")\n",
    "\n",
    "    os.startfile(os.getcwd().replace(\"\\\\\", \"/\") + \"/Output/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Process End ! Time spent: 0.61 mins\n"
     ]
    }
   ],
   "source": [
    "print(\"Analysis Process End ! Time spent: {time_spent} mins\".format(time_spent = round((time.time()-Program_Start_Time)/60, 2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PBI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
