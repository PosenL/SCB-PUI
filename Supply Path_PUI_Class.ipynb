{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Program_Start_Time  = time.time()\n",
    "print(\"Current file location: \"+ os.getcwd())\n",
    "print(\"Login WBI : \"+ os.getlogin())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Input Reference (PUI and JDA) and Do Data Preprocessing before Rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_preprocessing():\n",
    "    def __init__(self):\n",
    "        self.input_PUI_ref_path = r\"./Input/Supply_Chain_PUI.xlsx\"\n",
    "        self.input_PUI_JDA_path = r\"./Input/Input_JDA_List.xlsx\"\n",
    "    \n",
    "    def Data_loading(self):\n",
    "        print(\"File loading ....\")\n",
    "        #Record data loading time\n",
    "        Data_loading_Start_Time = time.time()\n",
    "        input_PUI_ref = pd.read_excel(self.input_PUI_ref_path, sheet_name=\"Raw\")\n",
    "        input_JDA_ref = pd.read_excel(self.input_PUI_JDA_path)\n",
    "        print(\"Finish data loading in {} mins.\".format(round((time.time() - Data_loading_Start_Time)/60, 2)))\n",
    "        return input_PUI_ref, input_JDA_ref\n",
    "    \n",
    "    def main(self):\n",
    "        df_PUI_raw, df_JDA_raw = self.Data_loading()\n",
    "        print(\"==============================================================\")\n",
    "        #Record processing time\n",
    "        Data_Preprocessing_Start_Time = time.time()\n",
    "        #Data preprocessing and aggregation\n",
    "        df_JDA_raw, MultiChip_dict = self.JDA_Input_Preprocessing(df_JDA_raw)\n",
    "        df_JDA_raw_ORIGIN = df_JDA_raw[[\"SC Index\", \"ITEM_NAME\", \"ITEM_CATEGORY\", \"JDA_PRODUCED_LOC_2\", \"CONSUMED_ITEM\", \"CONSUMED_ITEM_CATEGORY\", \"JDA_CONSUMED_LOC_2\", \"RAW_PLANT\"]].copy()\n",
    "\n",
    "        df_JDA_raw = df_JDA_raw.loc[df_JDA_raw[\"Excluded\"].isna()]\n",
    "        # df_JDA_raw = df_JDA_raw.loc[:200]\n",
    "        df_PUI_raw, df_agg, df_agg_final = self.Multichip_Decomposition(df_PUI_raw)\n",
    "        prod = self.Prod_Consumsed_Adjacency(df_agg_final)\n",
    "        Mtype = self.Mtype_Dict_Preprocessing(df_agg)\n",
    "        print(\"Finish data preprocessing in {} mins. Begin with data rolling...\\n\".format(round((time.time() - Data_Preprocessing_Start_Time)/60, 2)))\n",
    "        return prod, MultiChip_dict, df_JDA_raw_ORIGIN, df_JDA_raw, df_PUI_raw, df_agg, df_agg_final, Mtype\n",
    "    \n",
    "    def JDA_Input_Preprocessing(self, input_JDA_ref):\n",
    "        # Inset SC Index for JDA input (starting from 1)\n",
    "        input_JDA_ref[\"SC Index\"] = np.arange(1, len(input_JDA_ref)+1)\n",
    "        input_JDA_ref[\"FINAL START DATE\"] = input_JDA_ref[\"FINAL START DATE\"].astype(str).str.split(\" \", expand=True)[0]\n",
    "        input_JDA_ref[\"FINAL END DATE\"] = input_JDA_ref[\"FINAL END DATE\"].astype(str).str.split(\" \", expand=True)[0]\n",
    "        input_JDA_ref.insert(0, 'SC Index', input_JDA_ref.pop('SC Index'))\n",
    "\n",
    "        MultiChip_dict = {}\n",
    "        MultiChip_dict = dict(zip(input_JDA_ref[\"ITEM_BOM_RT_ID\"], input_JDA_ref[\"Multi Chip\"]))\n",
    "\n",
    "        # print(\"\\nData Schema of input JDA source: \")\n",
    "        # print(input_JDA_ref.info())\n",
    "        return input_JDA_ref, MultiChip_dict\n",
    "\n",
    "    def Multichip_Decomposition(self, input_PUI_ref):\n",
    "        # Transfer PUI table's index starting from 1\n",
    "        input_PUI_ref.index = np.arange(1, len(input_PUI_ref) + 1)\n",
    "\n",
    "        '''\n",
    "        Create a column MutiComp_index to save the index of ICAM -> CEXX_1, CEXX_2 in per decomposition record.\n",
    "        The record can be traced via the original input PUI frame to get the original multi-chip record\n",
    "        '''\n",
    "\n",
    "        # Create new column, COMP_12NC by copy \"1_12NC_LIST\" if that is not multi-chip (ignore ICFT), and transder the series type to string\n",
    "        input_PUI_ref[\"SingleComp_index\"] = input_PUI_ref.index\n",
    "        input_PUI_ref[\"COMP_12NC\"] = input_PUI_ref[\"1_12NC_LIST\"].copy()\n",
    "        input_PUI_ref.loc[(input_PUI_ref[\"COMP_12NC_LIST\"].str.contains(\",\")) & (input_PUI_ref[\"TYPE\"] == \"ICAM\"), \"COMP_12NC\"] = input_PUI_ref.loc[(input_PUI_ref[\"COMP_12NC_LIST\"].str.contains(\",\")) & (input_PUI_ref[\"TYPE\"] == \"ICAM\"), \"COMP_12NC_LIST\"]\n",
    "        input_PUI_ref[\"COMP_12NC\"] = input_PUI_ref[\"COMP_12NC\"].astype(str)\n",
    "\n",
    "        # Get component 12NC and extend them if they are multi-chip to an independent dataframe\n",
    "        df_connects_multi_CEXX = input_PUI_ref.loc[input_PUI_ref[\"COMP_12NC\"].str.contains(\",\")]\n",
    "        df_connects_multi_CEXX.loc[df_connects_multi_CEXX[\"COMP_12NC\"] .str.contains(\",\"), \"MutiComp\"] = df_connects_multi_CEXX.loc[df_connects_multi_CEXX[\"COMP_12NC\"].str.contains(\",\"), \"COMP_12NC\"]\n",
    "\n",
    "        df_connects_multi_CEXX[\"SingleComp_index\"] = pd.NaT\n",
    "        df_connects_multi_CEXX.loc[df_connects_multi_CEXX[\"COMP_12NC\"].str.contains(\",\"), \"MutiComp_index\"] = df_connects_multi_CEXX.loc[df_connects_multi_CEXX[\"COMP_12NC\"].str.contains(\",\")].index\n",
    "        df_connects_multi_CEXX[\"MutiComp_index\"] = df_connects_multi_CEXX[\"MutiComp_index\"].astype(int)\n",
    "\n",
    "        def split_rows(df, column):\n",
    "            # Create an empty DataFrame to store the new rows\n",
    "            new_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                # Check if the value in the specific column contains a comma\n",
    "                if \",\" in str(row[column]):\n",
    "                    # Split the value by comma\n",
    "                    split_values = str(row[column]).split(',')\n",
    "                    for value in split_values:\n",
    "                        # Create a new row with the split value\n",
    "                        new_row = row.copy()\n",
    "                        new_row[column] = value.strip()  # Remove any leading/trailing whitespace\n",
    "                        new_df = new_df.append(new_row, ignore_index=True)\n",
    "                else:\n",
    "                    # If no comma, just append the row as is\n",
    "                    new_df = new_df.append(row, ignore_index=True)\n",
    "            \n",
    "            return new_df\n",
    "\n",
    "        df_extend_multicomp = split_rows(df_connects_multi_CEXX, 'COMP_12NC')\n",
    "        df_extend_multicomp[\"COMP_12NC\"] = df_extend_multicomp[\"COMP_12NC\"].astype('str')\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        df_PUI_raw = original PUI table\n",
    "        df_extend_multicomp = independent dataframe to save the extended CEXX connects to ICAM where ICAM is multichip (per ICAM -> CEXX per row)\n",
    "        df_agg = Aggregated Result of df_PUI_raw and df_agg and reset the index.\n",
    "        df_agg_final = drop (ICAM -> CEXX_1, CEXX_2) in df_agg to make the data available for rolling. SingleComp_index, MutiComp_index can be the tracing index from df_agg\n",
    "\n",
    "        '''\n",
    "\n",
    "        df_agg = pd.concat([input_PUI_ref, df_extend_multicomp])\n",
    "        df_agg.reset_index(drop = True, inplace =True)\n",
    "        df_agg.index = np.arange(1, len(df_agg) + 1)\n",
    "\n",
    "        print(\"\\nLength of the Original PUI table: {}\".format(len(input_PUI_ref)))\n",
    "        print(\"Length of the Extended Multi-Component PUI table: {}\".format(len(df_extend_multicomp)))\n",
    "        print(\"Length of the Aggregated PUI table: {}, {}\".format(len(df_agg), len(df_agg) == (len(input_PUI_ref) + len(df_extend_multicomp))))\n",
    "\n",
    "        df_agg_final = df_agg[~df_agg.index.isin(df_agg[\"MutiComp_index\"].dropna())]\n",
    "\n",
    "        print(\"Length of the Aggreaged PUI and drop multiple list: {}\\n\".format(len(df_agg_final)))\n",
    "\n",
    "        # Dict for adjacent 12NC\n",
    "        df_agg_final[\"PART_12NC\"] = df_agg_final[\"PART_12NC\"].astype(\"int64\")\n",
    "        df_agg_final[\"COMP_12NC\"] = df_agg_final[\"COMP_12NC\"].astype(\"int64\")\n",
    "\n",
    "        # print(\"\\nData Schema Aggregated final PUI result: \")\n",
    "        # print(df_agg_final.info())\n",
    "        return input_PUI_ref, df_agg, df_agg_final\n",
    "    \n",
    "\n",
    "    def Prod_Consumsed_Adjacency(self, input_df_agg_final):\n",
    "        # Dict for adjacent 12NC\n",
    "        prod = {}\n",
    "        for x in input_df_agg_final[\"PART_12NC\"].unique():\n",
    "            prod[x] = input_df_agg_final.loc[input_df_agg_final['PART_12NC'] == x, \"COMP_12NC\"].to_list()\n",
    "        return prod\n",
    "\n",
    "    def Mtype_Dict_Preprocessing(self, input_df_agg):\n",
    "        # Dict for Matertial Type\n",
    "        material_12nc_type_list = [[\"PART_12NC\", \"TYPE\"],\n",
    "                                [\"1_12NC_LIST\", \"1_CLASS\"], \n",
    "                                [\"2_12NC_LIST\", \"2_CLASS\"], \n",
    "                                [\"3_12NC_LIST\", \"3_CLASS\"], \n",
    "                                [\"4_12NC_LIST\", \"4_CLASS\"]]\n",
    "        def Merge(dict1, dict2):\n",
    "            res = {**dict1, **dict2}\n",
    "            return res\n",
    "\n",
    "        Mtype = dict()\n",
    "        for material_12nc_type in (material_12nc_type_list):\n",
    "            sub_material = input_df_agg[material_12nc_type[0]].dropna()\n",
    "            sub_type = input_df_agg[material_12nc_type[1]].dropna()\n",
    "            sub_Mtype = dict(zip(sub_material,sub_type))\n",
    "            Mtype = Merge(Mtype, sub_Mtype)\n",
    "        \n",
    "        return Mtype\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PUI Structure Rolling Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PUI_Structure_Rolling:\n",
    "    def __init__(self):\n",
    "        self.PUI_anlyzied_slice_stack = pd.DataFrame([])\n",
    "        self.df_all_valid_supply_path = pd.DataFrame([])\n",
    "        self.PUI_stack = []\n",
    "\n",
    "    def main(self, input_SC_dataframe, input_PUI_dataframe, input_PUI_ref_before_drop_multi_comp):\n",
    "\n",
    "        print(\"Rolling Supply Path ===========================================================\")\n",
    "        for SC_Num, prod_item, consumed_item, consumed_plant, BOM_id in tqdm(zip(input_SC_dataframe[\"SC Index\"], input_SC_dataframe[\"ITEM_NAME\"], input_SC_dataframe[\"CONSUMED_ITEM\"], input_SC_dataframe[\"PLANT\"], input_SC_dataframe[\"ITEM_BOM_RT_ID\"]), total = len(input_SC_dataframe)):\n",
    "            all_paths_list = self.find_unique_path(prod, prod_item, consumed_item) # Get all applicable paths (prod to comp) -> list\n",
    "            if len(all_paths_list) == 0:\n",
    "                continue\n",
    "            self.df_all_valid_supply_path = pd.concat([self.df_all_valid_supply_path, self.Mtype_Aggregation(SC_Num, all_paths_list)]) # Get all applicable supply chain with Mtype Mapping -> dataframe\n",
    "            df_SC_analyezd_Stack = self.SC_Structure_Extraction(input_PUI_dataframe, all_paths_list, consumed_plant, BOM_id) # Extract each supply path and check if the supply path is valid\n",
    "            \n",
    "            #Result concating for each input supply path combination\n",
    "            df_SC_analyezd_Stack[\"SC Index\"] = SC_Num\n",
    "            self.PUI_anlyzied_slice_stack = pd.concat([self.PUI_anlyzied_slice_stack, df_SC_analyezd_Stack]) \n",
    "\n",
    "        if len(self.PUI_anlyzied_slice_stack) != 0:\n",
    "            self.PUI_anlyzied_slice_stack[\"Multi Chip\"] = self.PUI_anlyzied_slice_stack[\"ITEM_BOM_RT_ID\"].map(MultiChip_dict)\n",
    "            self.PUI_anlyzied_slice_stack[\"Produced_Item_Category\"] = self.PUI_anlyzied_slice_stack[\"Produced\"].map(Mtype)\n",
    "            self.PUI_anlyzied_slice_stack[\"Consumed_Item_Category\"] = self.PUI_anlyzied_slice_stack[\"Consumed\"].map(Mtype)\n",
    "            self.PUI_anlyzied_slice_stack.insert(0, 'SC Index', self.PUI_anlyzied_slice_stack.pop('SC Index'))\n",
    "            self.PUI_anlyzied_slice_stack.insert(2, 'ITEM_BOM_RT_ID', self.PUI_anlyzied_slice_stack.pop('ITEM_BOM_RT_ID'))\n",
    "            self.PUI_anlyzied_slice_stack.insert(3, \"Multi Chip\", self.PUI_anlyzied_slice_stack.pop(\"Multi Chip\"))\n",
    "            self.PUI_anlyzied_slice_stack.insert(5, \"Produced_Item_Category\", self.PUI_anlyzied_slice_stack.pop(\"Produced_Item_Category\"))\n",
    "            self.PUI_anlyzied_slice_stack.insert(7, \"Consumed_Item_Category\", self.PUI_anlyzied_slice_stack.pop(\"Consumed_Item_Category\"))\n",
    "\n",
    "        print(\"\\nMulti-Component Checking ======================================================\")\n",
    "        df_component_analyzed_result  = self.MultiComponent_Checking(self.PUI_anlyzied_slice_stack, input_SC_dataframe)\n",
    "\n",
    "        print(\"\\nPUI Mapping Result ============================================================\")\n",
    "        df_PUI_mapping = self.PUI_result_mapping(input_PUI_ref_before_drop_multi_comp, df_component_analyzed_result)\n",
    "        \n",
    "        #Result proprocessing\n",
    "        if len(df_PUI_mapping) != 0:\n",
    "            # Mapping JDA columns into PUI: [\"JDA_PRODUCED_LOC_2\", \"JDA_CONSUMED_LOC_2\", \"FINAL START DATE\", \"FINAL END DATE\"]\n",
    "            input_SC_dataframe_to_be_mapped = input_SC_dataframe[[\"SC Index\", \"JDA_PRODUCED_LOC_2\", \"JDA_CONSUMED_LOC_2\", \"FINAL START DATE\", \"FINAL END DATE\"]]\n",
    "            df_PUI_mapping = df_PUI_mapping.merge(input_SC_dataframe_to_be_mapped, on='SC Index', how='left')\n",
    "\n",
    "            # Column format and order change\n",
    "            df_PUI_mapping[\"PART_12NC\"] = df_PUI_mapping[\"PART_12NC\"].astype(str)\n",
    "            df_PUI_mapping[\"COMP_12NC\"] = df_PUI_mapping[\"COMP_12NC\"].astype(str)\n",
    "            df_PUI_mapping.insert(0, 'PUI_Index', df_PUI_mapping.pop('PUI_Index'))\n",
    "            df_PUI_mapping.insert(1, 'SC Index', df_PUI_mapping.pop('SC Index'))\n",
    "            df_PUI_mapping.insert(2, 'ITEM_BOM_RT_ID', df_PUI_mapping.pop('ITEM_BOM_RT_ID'))\n",
    "            df_PUI_mapping.insert(3, 'JDA_PRODUCED_LOC_2', df_PUI_mapping.pop('JDA_PRODUCED_LOC_2'))\n",
    "            df_PUI_mapping.insert(4, 'JDA_CONSUMED_LOC_2', df_PUI_mapping.pop('JDA_CONSUMED_LOC_2'))\n",
    "            df_PUI_mapping.insert(5, 'FINAL START DATE', df_PUI_mapping.pop('FINAL START DATE'))\n",
    "            df_PUI_mapping.insert(6, 'FINAL END DATE', df_PUI_mapping.pop('FINAL END DATE'))\n",
    "            df_PUI_mapping.insert(10, 'COMP_12NC', df_PUI_mapping.pop('COMP_12NC'))\n",
    "\n",
    "            # Drop unneccsary columns\n",
    "            df_PUI_mapping.drop(columns = [\"SingleComp_index\", \"MutiComp\", \"MutiComp_index\", \"EFF_FR_DATE\", \"EFF_TO_DATE\"], inplace = True)\n",
    "            df_PUI_mapping.reset_index(drop= True, inplace = True)\n",
    "\n",
    "        #Mapping two results back to JDA input\n",
    "        input_SC_dataframe[\"Connected Rolling Structure\"] = \"No\"\n",
    "        input_SC_dataframe[\"Valid Rolling Structure\"] = \"No\"\n",
    "        if len(df_component_analyzed_result) != 0:\n",
    "            input_SC_dataframe.loc[input_SC_dataframe[\"ITEM_BOM_RT_ID\"].isin(df_component_analyzed_result[\"ITEM_BOM_RT_ID\"]), \"Connected Rolling Structure\"] = \"Yes\"\n",
    "            input_SC_dataframe.loc[input_SC_dataframe[\"ITEM_BOM_RT_ID\"].isin(df_component_analyzed_result.loc[df_component_analyzed_result[\"Valid_Comp_Combination\"] == True, \"ITEM_BOM_RT_ID\"].unique()), \"Valid Rolling Structure\"] = \"Yes\"\n",
    "\n",
    "        return df_PUI_mapping, df_component_analyzed_result, self.df_all_valid_supply_path, input_SC_dataframe\n",
    "\n",
    "    def find_all_paths(self, graph, start, end, path=None, unique_paths=None):\n",
    "        if path is None:\n",
    "            path = []\n",
    "        if unique_paths is None:\n",
    "            unique_paths = set()\n",
    "        path.append(start)\n",
    "        if start == end:\n",
    "            # Convert path to a tuple so it can be added to a set\n",
    "            unique_paths.add(tuple(path))\n",
    "        else:\n",
    "            for node in graph.get(start, []):\n",
    "                if node not in path:  # Avoid cycles\n",
    "                    self.find_all_paths(graph, node, end, path.copy(), unique_paths)\n",
    "        return unique_paths\n",
    "\n",
    "\n",
    "    def find_unique_path(self, graph, start, end, path=None, unique_paths=None):\n",
    "        # Find all paths\n",
    "        all_paths = self.find_all_paths(graph, start, end)\n",
    "        # Convert each tuple path back to a list if needed\n",
    "        all_paths = [list(path) for path in all_paths]\n",
    "        return all_paths\n",
    "\n",
    "\n",
    "    def Mtype_Aggregation(self, SC_Num, input_all_paths):\n",
    "        Info = []\n",
    "        for x in input_all_paths:\n",
    "            Info_sub = []\n",
    "            for y in x:\n",
    "                Info_sub.append(Mtype[y])\n",
    "            Info.append(Info_sub)\n",
    "\n",
    "        #Output\n",
    "        stacker = []\n",
    "        for x, y in zip(Info, input_all_paths):\n",
    "            stacker.append(x)\n",
    "            stacker.append(y)\n",
    "        output_df = pd.DataFrame(stacker)\n",
    "        output_df.columns = [('Element_' + str(x + 1)) for x in range(len(output_df.columns))]\n",
    "        output_df[\"SC Index\"] = SC_Num\n",
    "        output_df.insert(0, 'SC Index', output_df.pop('SC Index'))\n",
    "        return output_df\n",
    "\n",
    "    def SC_Structure_Extraction(self, input_PUI_ref, PUI_valid_path_list, consumed_plant, BOM_id):\n",
    "        SC_Stack = []\n",
    "        idx = 0\n",
    "        \n",
    "        for x in PUI_valid_path_list:\n",
    "            idx +=1\n",
    "            for y in range(len(x)):\n",
    "                try:\n",
    "                    SC_Stack.append([idx, x[y], x[y+1], input_PUI_ref.loc[(input_PUI_ref[\"PART_12NC\"] == x[y]) & (input_PUI_ref[\"COMP_12NC\"] == x[y+1]), \"PLANT\"].unique()])\n",
    "                except:\n",
    "                    # SC_Stack.append([\"\", \"\", \"\", \"\"])\n",
    "                    continue\n",
    "\n",
    "            \n",
    "        df_SC_analyezd_Stack = pd.DataFrame(SC_Stack, columns = [\"Rolling Result - Combination\", \"Produced\", \"Consumed\", \"Site\"])\n",
    "        #Check if target plant is in Available Plant for each combination\n",
    "        df_SC_analyezd_Stack[\"Target Plant\"] = consumed_plant\n",
    "        df_SC_analyezd_Stack[\"ITEM_BOM_RT_ID\"] = BOM_id\n",
    "        df_SC_analyezd_Stack[\"Available Plant\"] = df_SC_analyezd_Stack.apply(lambda x: True if consumed_plant in x[\"Site\"] else False, axis = 1)\n",
    "\n",
    "\n",
    "        for c in df_SC_analyezd_Stack[\"Rolling Result - Combination\"].unique():\n",
    "            if c != '':\n",
    "                each_comb = df_SC_analyezd_Stack.loc[(df_SC_analyezd_Stack[\"Rolling Result - Combination\"] == c)]\n",
    "                if (each_comb[\"Available Plant\"] == False).any() == True:\n",
    "                    df_SC_analyezd_Stack.loc[(df_SC_analyezd_Stack[\"Rolling Result - Combination\"] == c), \"Valid_Plant\"] = False\n",
    "                else:\n",
    "                    df_SC_analyezd_Stack.loc[(df_SC_analyezd_Stack[\"Rolling Result - Combination\"] == c), \"Valid_Plant\"] = True\n",
    "\n",
    "        df_SC_analyezd_Stack.fillna(\"\", inplace = True)\n",
    "        return df_SC_analyezd_Stack\n",
    "\n",
    "    def MultiComponent_Checking(self, input_PUI_anlyzied_slice_stack, input_SC_dataframe):\n",
    "        if len(input_PUI_anlyzied_slice_stack) == 0:\n",
    "            return pd.DataFrame([])\n",
    "        else:\n",
    "            input_PUI_anlyzied_slice_stack[\"Valid_Comp_Combination\"] = False\n",
    "            # Initialize all records are invalid in the beginning \n",
    "            for bom_id in tqdm(input_PUI_anlyzied_slice_stack[\"ITEM_BOM_RT_ID\"].unique(), total = len(input_PUI_anlyzied_slice_stack[\"ITEM_BOM_RT_ID\"].unique())):\n",
    "                df_extract_from_bom_id = input_PUI_anlyzied_slice_stack.loc[(input_PUI_anlyzied_slice_stack[\"ITEM_BOM_RT_ID\"] == bom_id)]\n",
    "                \n",
    "                # Check if the provided number of SC != the number of rolling out SC Index\n",
    "                if len(df_extract_from_bom_id[\"SC Index\"].unique()) != len(input_SC_dataframe.loc[input_SC_dataframe[\"ITEM_BOM_RT_ID\"] == bom_id]):\n",
    "                    input_PUI_anlyzied_slice_stack.loc[(input_PUI_anlyzied_slice_stack[\"ITEM_BOM_RT_ID\"] == bom_id), \"Valid_Comp_Combination\"] = False\n",
    "                    continue\n",
    "                else:\n",
    "                    if sum(df_extract_from_bom_id[\"Multi Chip\"] == \"No\") == 0:\n",
    "                        #Check if it marked as Mutichip but only one SC is input\n",
    "                        if len(input_SC_dataframe.loc[input_SC_dataframe[\"ITEM_BOM_RT_ID\"] == bom_id]) == 1:\n",
    "                            continue\n",
    "\n",
    "                        # Get count of ICAM\n",
    "                        ICAM_count_per_SC_index = df_extract_from_bom_id.loc[df_extract_from_bom_id[\"Produced_Item_Category\"] ==\"ICAM\", [\"SC Index\", \"Produced\"]].drop_duplicates().value_counts(\"Produced\")\n",
    "                        Valid_ICAM_list = ICAM_count_per_SC_index[ICAM_count_per_SC_index >= len(input_SC_dataframe.loc[input_SC_dataframe[\"ITEM_BOM_RT_ID\"] == bom_id])].keys().to_list()\n",
    "                        for valid_ICAM in Valid_ICAM_list:\n",
    "                            # Get all valid ICAM combination (SC_Index, Combination)\n",
    "                            valid_ICAM_comb = df_extract_from_bom_id.loc[df_extract_from_bom_id[\"Produced\"] == valid_ICAM, [\"SC Index\", \"Rolling Result - Combination\"]]\n",
    "                            valid_ICAM_comb_mapping_result = df_extract_from_bom_id.merge(valid_ICAM_comb, how='inner', on = [\"SC Index\", \"Rolling Result - Combination\"])\n",
    "                            Checking_list_per_SC_index = []\n",
    "                            \n",
    "                            # Check if per SC combination has its valid ICAM\n",
    "                            for val_plant_checking_per_SC_index in valid_ICAM_comb_mapping_result[\"SC Index\"].unique():\n",
    "                                if sum(valid_ICAM_comb_mapping_result.loc[(valid_ICAM_comb_mapping_result[\"SC Index\"] == val_plant_checking_per_SC_index), \"Valid_Plant\"] == True) != 0:\n",
    "                                    if val_plant_checking_per_SC_index not in Checking_list_per_SC_index:\n",
    "                                        Checking_list_per_SC_index.append(val_plant_checking_per_SC_index)\n",
    "                            \n",
    "                            if len(Checking_list_per_SC_index) == len(valid_ICAM_comb[\"SC Index\"].unique()):\n",
    "                                for sc_idx, roll_comb in valid_ICAM_comb.values:\n",
    "                                    if sum(valid_ICAM_comb_mapping_result.loc[(valid_ICAM_comb_mapping_result[\"SC Index\"] == sc_idx) & (valid_ICAM_comb_mapping_result[\"Rolling Result - Combination\"] == roll_comb), \"Valid_Plant\"] == False) == 0:\n",
    "                                        input_PUI_anlyzied_slice_stack.loc[(input_PUI_anlyzied_slice_stack[\"SC Index\"] == sc_idx) & (input_PUI_anlyzied_slice_stack[\"Rolling Result - Combination\"] == roll_comb), \"Valid_Comp_Combination\"] = True\n",
    "                    else:\n",
    "                        for comb_id in df_extract_from_bom_id[\"Rolling Result - Combination\"].unique():\n",
    "                            df_extract_from_bom_id_and_comb_id = df_extract_from_bom_id[df_extract_from_bom_id[\"Rolling Result - Combination\"] == comb_id]\n",
    "                            if (df_extract_from_bom_id_and_comb_id[\"Valid_Plant\"] == False).any():\n",
    "                                input_PUI_anlyzied_slice_stack.loc[(input_PUI_anlyzied_slice_stack[\"ITEM_BOM_RT_ID\"] == bom_id) & (input_PUI_anlyzied_slice_stack[\"Rolling Result - Combination\"] == comb_id), \"Valid_Comp_Combination\"] = False\n",
    "                            else:\n",
    "                                input_PUI_anlyzied_slice_stack.loc[(input_PUI_anlyzied_slice_stack[\"ITEM_BOM_RT_ID\"] == bom_id) & (input_PUI_anlyzied_slice_stack[\"Rolling Result - Combination\"] == comb_id), \"Valid_Comp_Combination\"] = True\n",
    "            return input_PUI_anlyzied_slice_stack\n",
    "\n",
    "    def PUI_result_mapping(self, input_PUI_ref_before_drop_multi_comp, input_SC_analyezd_Stack):\n",
    "        if len(input_SC_analyezd_Stack) == 0:\n",
    "            return pd.DataFrame([])\n",
    "        else:\n",
    "            for idx, bom_id, p, c, plt, val in tqdm(zip(input_SC_analyezd_Stack[\"SC Index\"], input_SC_analyezd_Stack[\"ITEM_BOM_RT_ID\"], input_SC_analyezd_Stack[\"Produced\"], input_SC_analyezd_Stack[\"Consumed\"], input_SC_analyezd_Stack[\"Target Plant\"], input_SC_analyezd_Stack[\"Valid_Comp_Combination\"]), total = len(input_SC_analyezd_Stack[\"SC Index\"])):\n",
    "                if val == True:\n",
    "                    sub_PUI_index_single = input_PUI_ref_before_drop_multi_comp.loc[(input_PUI_ref_before_drop_multi_comp[\"PART_12NC\"] == p) & (input_PUI_ref_before_drop_multi_comp[\"COMP_12NC\"] == str(c)) & (input_PUI_ref_before_drop_multi_comp[\"PLANT\"] == plt), \"SingleComp_index\"].dropna().values\n",
    "                    sub_PUI_index_multi = input_PUI_ref_before_drop_multi_comp.loc[(input_PUI_ref_before_drop_multi_comp[\"PART_12NC\"] == p) & (input_PUI_ref_before_drop_multi_comp[\"COMP_12NC\"] == str(c)) & (input_PUI_ref_before_drop_multi_comp[\"PLANT\"] == plt), \"MutiComp_index\"].dropna().values\n",
    "                    sub_PUI_index = set(sub_PUI_index_single).union(set(sub_PUI_index_multi))\n",
    "\n",
    "                    sub_PUI = input_PUI_ref_before_drop_multi_comp.loc[sub_PUI_index]\n",
    "                    sub_PUI[\"SC Index\"] = idx\n",
    "                    sub_PUI[\"ITEM_BOM_RT_ID\"] = bom_id\n",
    "                    \n",
    "                   \n",
    "                    if len(sub_PUI_index) != 0:\n",
    "                        self.PUI_stack.append(sub_PUI)\n",
    "\n",
    "            if len(self.PUI_stack) == 0:\n",
    "                return pd.DataFrame([])\n",
    "            else:\n",
    "                PUI_mapping_result = pd.concat(self.PUI_stack)\n",
    "                # Create an index for PUImapping result checking but keep the original PUI index\n",
    "                PUI_mapping_result[\"PUI_Index\"] = PUI_mapping_result.index\n",
    "                PUI_mapping_result[\"Index for Mapping Result Validation\"] = np.arange(1, len(PUI_mapping_result) + 1) \n",
    "                PUI_mapping_result.to_csv(\"PUI_Mapping_Checking_20240812.csv\")\n",
    "                print(\"\\nPUI Mapping Result Validation =================================================\")\n",
    "                return self.PUI_result_mapping_validate(PUI_mapping_result)\n",
    "\n",
    "    def PUI_result_mapping_validate(self, input_PUI_mapping_result):\n",
    "        all_index_to_be_removed = []\n",
    "        multi_comp_bom_id_list= input_PUI_mapping_result.loc[input_PUI_mapping_result[\"COMP_12NC\"].str.contains(\",\"), \"ITEM_BOM_RT_ID\"].unique()\n",
    "        for x in tqdm(multi_comp_bom_id_list, total = len(multi_comp_bom_id_list)):\n",
    "            removed_index_per_bom_id = []\n",
    "            s = input_PUI_mapping_result.loc[(input_PUI_mapping_result[\"ITEM_BOM_RT_ID\"] == x) & (input_PUI_mapping_result[\"TYPE\"] == \"ICAM\") & (input_PUI_mapping_result[\"COMP_12NC\"].str.contains(\",\")) & (input_PUI_mapping_result[\"1_CLASS\"] == \"CESF\")]\n",
    "            s_dict = s[\"COMP_12NC\"].value_counts().to_dict()\n",
    "            for (k, v) in s_dict.items():\n",
    "                if (v % len(k.split(\",\"))) != 0:\n",
    "                    removed_index_per_bom_id.append(s.loc[s[\"COMP_12NC\"] == k, \"Index for Mapping Result Validation\"].tolist())\n",
    "            removed__all_index_per_bom_id = list(itertools.chain(*removed_index_per_bom_id))\n",
    "            if len(removed__all_index_per_bom_id) != 0:\n",
    "                all_index_to_be_removed.append(removed__all_index_per_bom_id)\n",
    "        return input_PUI_mapping_result.loc[~input_PUI_mapping_result[\"Index for Mapping Result Validation\"].isin(list(itertools.chain(*all_index_to_be_removed)))].drop(\"Index for Mapping Result Validation\", axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turnkey Check after Rolling and Broken Flow Checking for the input 12NCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Turnkey_Broken_Validation:\n",
    "    def __init__(self):\n",
    "        self.turnkey_mtype_list = [\"SLPD\", \"CEPT\"]\n",
    "        self.turnkey_list_all = []\n",
    "        self.broken_list_dict = {}\n",
    "\n",
    "    def main(self, input_JDA_frame_origin, input_JDA_frame_aggregated, input_rolling_map):\n",
    "        print(\"\\nTurnkey Checking ... ==========================================================\")\n",
    "        input_turnkey_frame = self.Turnkey_Checking(input_rolling_map)\n",
    "        print(\"\\nBroken Flow Checking ... ======================================================\")\n",
    "        input_broken_dict = self.Broken_Flow_Checking(input_JDA_frame_origin)\n",
    "\n",
    "        aggregate_frame = input_JDA_frame_aggregated.merge(input_turnkey_frame, on='SC Index', how='left')\n",
    "        input_JDA_frame_origin[\"Broken_Flow\"] = input_JDA_frame_origin[\"SC Index\"].map(input_broken_dict)\n",
    "        input_JDA_frame_origin = input_JDA_frame_origin[[\"SC Index\", \"Broken_Flow\", \"ITEM_NAME\", \"ITEM_CATEGORY\", \"JDA_PRODUCED_LOC_2\", \"CONSUMED_ITEM\", \"CONSUMED_ITEM_CATEGORY\", \"JDA_CONSUMED_LOC_2\", \"RAW_PLANT\"]]\n",
    "        input_JDA_frame_origin.rename(columns = {\"RAW_PLANT\":\"RAW_PLANT (CONSUMED)\", }, inplace =True)\n",
    "        return aggregate_frame, input_JDA_frame_origin\n",
    "\n",
    "    def Turnkey_Checking(self, input_rolling_map):\n",
    "        for SC_idx in tqdm(input_rolling_map.loc[input_rolling_map[\"Valid_Comp_Combination\"] == True, \"SC Index\"].unique()):\n",
    "            SC_valid_rolling_frame = input_rolling_map.loc[input_rolling_map[\"SC Index\"] == SC_idx]\n",
    "            SC_valid_rolling_frame_list = SC_valid_rolling_frame[[\"Produced\", \"Produced_Item_Category\"]].values.tolist() + SC_valid_rolling_frame[[\"Consumed\", \"Consumed_Item_Category\"]].values.tolist()           \n",
    "            \n",
    "            turnkey_indicator = False\n",
    "            # Check if turnkey the predefined turnkey is included and not the produced/consumed items\n",
    "            for turnkey_mtype in self.turnkey_mtype_list:\n",
    "                if turnkey_mtype in list(itertools.chain(*SC_valid_rolling_frame_list)):\n",
    "                    if turnkey_mtype not in [SC_valid_rolling_frame_list[0][1], SC_valid_rolling_frame_list[-1][1]]:\n",
    "                        turnkey_indicator = True\n",
    "                        break\n",
    "\n",
    "            if turnkey_indicator == True:\n",
    "                turnkey_frame = pd.DataFrame(SC_valid_rolling_frame_list, columns = [\"12NC\", \"Mtype\"])\n",
    "                turnkey_SLPD_set = set(turnkey_frame.loc[turnkey_frame[\"Mtype\"] == \"SLPD\", \"12NC\"])\n",
    "                turnkey_CEPT_set = set(turnkey_frame.loc[turnkey_frame[\"Mtype\"] == \"CEPT\", \"12NC\"])\n",
    "\n",
    "                turnkey_SLPD_str = \", \".join(map(str, turnkey_SLPD_set))\n",
    "                turnkey_CEPT_str = \", \".join(map(str, turnkey_CEPT_set))\n",
    "                self.turnkey_list_all.append([SC_idx, turnkey_SLPD_str, turnkey_CEPT_str])\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "        return pd.DataFrame(self.turnkey_list_all, columns = [\"SC Index\", \"Turnkey_SLPD\", \"Turnkey_CEPT\"])\n",
    "    \n",
    "    def Broken_Flow_Checking(self, input_JDA_frame_origin):\n",
    "        for SC_idx, prod_12NC, prod_Mtype, prod_loc in tqdm(zip(input_JDA_frame_origin[\"SC Index\"], input_JDA_frame_origin[\"ITEM_NAME\"], input_JDA_frame_origin[\"ITEM_CATEGORY\"], input_JDA_frame_origin[\"JDA_PRODUCED_LOC_2\"]), total = len(input_JDA_frame_origin)):\n",
    "            if prod_Mtype in self.turnkey_mtype_list:\n",
    "                self.broken_list_dict[SC_idx] = False\n",
    "            else:\n",
    "                if len(input_JDA_frame_origin.loc[(input_JDA_frame_origin[\"CONSUMED_ITEM\"] == prod_12NC) & (input_JDA_frame_origin[\"JDA_CONSUMED_LOC_2\"] == prod_loc)]) != 0:\n",
    "                    self.broken_list_dict[SC_idx] = True\n",
    "                else:\n",
    "                    self.broken_list_dict[SC_idx] = False\n",
    "        return self.broken_list_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Checking and Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from styleframe import StyleFrame, Styler, utils\n",
    "\n",
    "class General_function:\n",
    "     def __init__(self):\n",
    "          self.today = (datetime.datetime.today()).strftime('%Y%m%d')\n",
    "          self.output_path = \"./Output/Result_table_{}.xlsx\".format(self.today)\n",
    "\n",
    "     def Export_to_excel_with_style(self, input_Result, input_Result_SP, input_Reuslt_PUI_anlyzied_slice, input_Result_Broken_Flow_source, input_Result_JDA_source):\n",
    "          try:\n",
    "               input_Result_drop_dup = input_Result.drop(columns=[\"SC Index\", \"ITEM_BOM_RT_ID\"]).drop_duplicates().sort_values(by = \"PUI_Index\")\n",
    "          except:\n",
    "               input_Result_drop_dup = input_Result.copy()\n",
    "\n",
    "          Result_drop_dup_frame = General_function().style_changes(input_Result_drop_dup, \n",
    "                                                          [[\"PUI_Index\"], [\"COMP_12NC\"], [\"PLANT\"], [\"JDA_PRODUCED_LOC_2\", \"JDA_CONSUMED_LOC_2\", \"FINAL START DATE\", \"FINAL END DATE\"]],\n",
    "                                                          [\"yellow\", \"#85C1E9\", \"green\", \"#ADADAD\"]\n",
    "                                                          )\n",
    "          \n",
    "          Result_frame = General_function().style_changes(input_Result, \n",
    "                                                          [[\"PUI_Index\", \"SC Index\", \"ITEM_BOM_RT_ID\"], [\"COMP_12NC\"], [\"PLANT\"], [\"JDA_PRODUCED_LOC_2\", \"JDA_CONSUMED_LOC_2\", \"FINAL START DATE\", \"FINAL END DATE\"]],\n",
    "                                                          [\"yellow\", \"#85C1E9\", \"green\", \"#ADADAD\"]\n",
    "                                                          )\n",
    "          Result_SP_frame = General_function().style_changes(input_Result_SP.reset_index(drop = True),\n",
    "                                                            [\n",
    "                                                               [\"SC Index\", \"Rolling Result - Combination\"], \n",
    "                                                               [\"Produced\", \"Produced_Item_Category\", \"Consumed\", \"Consumed_Item_Category\"], \n",
    "                                                               [\"Target Plant\"], \n",
    "                                                               [\"ITEM_BOM_RT_ID\"]\n",
    "                                                            ],\n",
    "                                                            [\"yellow\", \"#85C1E9\", \"green\", ]\n",
    "                                                          )\n",
    "\n",
    "          Reuslt_PUI_anlyzied_slice_frame = General_function().style_changes(input_Reuslt_PUI_anlyzied_slice.reset_index(drop = True),\n",
    "                                                          [[\"SC Index\"]],\n",
    "                                                          [\"yellow\"]\n",
    "                                                          )\n",
    "          Result_Broken_Flow_source_frame = General_function().style_changes(input_Result_Broken_Flow_source,\n",
    "                                                          [[\"SC Index\", \"Broken_Flow\", \"RAW_PLANT (CONSUMED)\"]],\n",
    "                                                          [\"yellow\"]\n",
    "                                                          )\n",
    "          input_Result_JDA_source_frame = General_function().style_changes(input_Result_JDA_source,\n",
    "                                                            [\n",
    "                                                                 [\"SC Index\", \"Connected Rolling Structure\", \"Valid Rolling Structure\"], \n",
    "                                                                 [\"ITEM_NAME\", \"ITEM_CATEGORY\", \"ITEM_DESCRIPTION\", \"PRODUCED_LOCATION\", \"JDA_PRODUCED_LOC_2\"],\n",
    "                                                                 [\"CONSUMED_ITEM\", \"CONSUMED_ITEM_CATEGORY\", \"CONSUMED_ITEM_DESCRIPTION\", \"RAW_PLANT\", \"PLANT\", \"JDA_CONSUMED_LOC_2\"],\n",
    "                                                                 [\"Excluded\"],\n",
    "                                                                 [\"Turnkey_SLPD\", \"Turnkey_CEPT\"],\n",
    "                                                                 [\"FINAL START DATE\",\"FINAL END DATE\"]\n",
    "                                                             ],\n",
    "                                                             [\"yellow\", \"#85C1E9\", \"green\", \"#7D7DFF\", \"#1AFD9C\", \"#ADADAD\"]\n",
    "                                                            )\n",
    "          \n",
    "\n",
    "          #Export to excel\n",
    "          excel_writer = StyleFrame.ExcelWriter(self.output_path)\n",
    "          Result_drop_dup_frame.to_excel(excel_writer, sheet_name = \"Result - PUI Mapping (Drop_dup)\", index= False, header = True)\n",
    "          Result_frame.to_excel(excel_writer, sheet_name = \"Result - PUI Mapping\", index= False, header = True)\n",
    "          Result_SP_frame.to_excel(excel_writer, sheet_name = \"Analyzed - Rolling Supply Path\", index= False, header = True)\n",
    "          Reuslt_PUI_anlyzied_slice_frame.to_excel(excel_writer, sheet_name = \"Initial - Rolling Supply Chain\", index= False, header = True)\n",
    "          Result_Broken_Flow_source_frame.to_excel(excel_writer, sheet_name = \"Initial - Broken Flow Checking\", index= False, header = True)\n",
    "          input_Result_JDA_source_frame.to_excel(excel_writer, sheet_name = \"Input - JDA Source\", index= False, header = True)\n",
    "          excel_writer.save()\n",
    "\n",
    "          return \"Data exported completedly !\"\n",
    "          \n",
    "     def style_changes(self, input_frame, update_columns_combination, color_combination):\n",
    "          # Reformat and export the excel file with color and font style\n",
    "          if len(input_frame) == 0:\n",
    "               return StyleFrame(pd.DataFrame([]))\n",
    "          else:\n",
    "               sf_output_ref = StyleFrame(input_frame)\n",
    "               for h_col in input_frame.columns:\n",
    "                    sf_output_ref[h_col] = input_frame[h_col].fillna(\"\").values\n",
    "                    sf_output_ref[h_col] = sf_output_ref[h_col].astype(str)\n",
    "                    header_width = input_frame[h_col].astype(str).str.len().max() + 8\n",
    "                    if header_width <= len(h_col):\n",
    "                         header_width = len(h_col) + 12\n",
    "                    sf_output_ref.apply_column_style(cols_to_style = h_col,\n",
    "                                                  styler_obj=Styler(font=utils.fonts.calibri, font_size= 11),\n",
    "                                                  width=header_width,\n",
    "                                                  style_header=False\n",
    "                                                  )\n",
    "\n",
    "                    \n",
    "               sf_output_ref.apply_headers_style(styler_obj = Styler(font = 'Calibri', font_size = 11, bg_color = \"#FDEBD0\"), cols_to_style = input_frame.columns)\n",
    "               for col_list, color in zip(update_columns_combination, color_combination):\n",
    "                    sf_output_ref.apply_headers_style(styler_obj = Styler(font = 'Calibri', font_size = 11, bg_color = color), cols_to_style = col_list)\n",
    "               return sf_output_ref     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution  - Trigger all Flows Working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__=='__main__':\n",
    "    prod, MultiChip_dict, df_JDA_origin, df_JDA_input, df_PUI_raw, df_agg, df_agg_final, Mtype = Data_preprocessing().main()\n",
    "    Result, Result_SP, Reuslt_PUI_anlyzied_slice, Result_JDA_source = PUI_Structure_Rolling().main(df_JDA_input, df_agg_final, df_agg)\n",
    "    Result_JDA_output, Result_JDA_frame_broken= Turnkey_Broken_Validation().main(df_JDA_origin, df_JDA_input, Result_SP)\n",
    "\n",
    "    # Export to excel with style\n",
    "    print(\"\\n>> Result is now exporting to excel files ...... \")\n",
    "    Excel_Exportint_Start_Time = time.time()\n",
    "    General_function().Export_to_excel_with_style(Result, Result_SP, Reuslt_PUI_anlyzied_slice, Result_JDA_frame_broken, Result_JDA_output)\n",
    "    print(\">> Export Process is now completed in {} mins!\".format(round((time.time()-Excel_Exportint_Start_Time)/60, 2)))\n",
    "\n",
    "    os.startfile(os.getcwd().replace(\"\\\\\", \"/\") + \"/Output/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analysis Process End ! Time spent: {time_spent} mins\".format(time_spent = round((time.time()-Program_Start_Time)/60, 2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PBI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
