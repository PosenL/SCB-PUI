{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from itertools import chain\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current file location: c:\\Planning S&P\\Project\\SCB_PUI\n",
      "Login WBI : nxf83451\n"
     ]
    }
   ],
   "source": [
    "Program_Start_Time  = time.time()\n",
    "print(\"Current file location: \"+ os.getcwd())\n",
    "print(\"Login WBI : \"+ os.getlogin())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Input Reference (PUI and JDA) and Do Data Preprocessing before Rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_preprocessing():\n",
    "    def __init__(self):\n",
    "        self.input_PUI_ref_path = r\"./Input/Supply_Chain_PUI.xlsx\"\n",
    "        self.input_PUI_JDA_path = r\"./Input/Input_JDA_List.xlsx\"\n",
    "    \n",
    "    def Date_loading(self):\n",
    "        print(\"File loading ....\")\n",
    "        #Record data loading time\n",
    "        Data_loading_Start_Time = time.time()\n",
    "        input_PUI_ref = pd.read_excel(self.input_PUI_ref_path, sheet_name=\"Raw\")\n",
    "        input_JDA_ref = pd.read_excel(self.input_PUI_JDA_path)\n",
    "        print(\"Finish data loading in {} mins.\".format(round((time.time() - Data_loading_Start_Time)/60, 2)))\n",
    "        return input_PUI_ref, input_JDA_ref\n",
    "    \n",
    "    def main(self):\n",
    "        df_PUI_raw, df_JDA_raw = self.Date_loading()\n",
    "\n",
    "        print(\"==============================================================\")\n",
    "        #Record processing time\n",
    "        Data_Preprocessing_Start_Time = time.time()\n",
    "\n",
    "        #Data preprocessing and aggregation\n",
    "        df_JDA_raw, MultiChip_dict = self.JDA_Input_Preprocessing(df_JDA_raw)\n",
    "        df_PUI_raw, df_agg, df_agg_final = self.Multichip_Decomposition(df_PUI_raw)\n",
    "        prod = self.Prod_Consumsed_Adjacency(df_agg_final)\n",
    "        Mtype = self.Mtype_Dict_Preprocessing(df_agg)\n",
    "        print(\"Finish data preprocessing in {} mins. Begin with data rolling...\".format(round((time.time() - Data_Preprocessing_Start_Time)/60, 2)))\n",
    "        return prod, MultiChip_dict, df_JDA_raw, df_PUI_raw, df_agg, df_agg_final, Mtype\n",
    "    \n",
    "    def JDA_Input_Preprocessing(self, input_JDA_ref):\n",
    "        # Inset SC Index for JDA input (starting from 1)\n",
    "        input_JDA_ref[\"SC Index\"] = np.arange(1, len(input_JDA_ref)+1)\n",
    "        input_JDA_ref.insert(0, 'SC Index', input_JDA_ref.pop('SC Index'))\n",
    "\n",
    "        MultiChip_dict = {}\n",
    "        MultiChip_dict = dict(zip(input_JDA_ref[\"ITEM_BOM_RT_ID\"], input_JDA_ref[\"Multi Chip\"]))\n",
    "\n",
    "        # print(\"\\nData Schema of input JDA source: \")\n",
    "        # print(input_JDA_ref.info())\n",
    "        return input_JDA_ref, MultiChip_dict\n",
    "\n",
    "    def Multichip_Decomposition(self, input_PUI_ref):\n",
    "        input_PUI_ref[\"EFF_FR_DATE\"] = input_PUI_ref[\"EFF_FR_DATE\"].astype(str).str.split(\" \", expand=True)[0]\n",
    "        input_PUI_ref[\"EFF_TO_DATE\"] = input_PUI_ref[\"EFF_TO_DATE\"].astype(str).str.split(\" \", expand=True)[0]\n",
    "\n",
    "        # Transfer PUI table's index starting from 1\n",
    "        input_PUI_ref.index = np.arange(1, len(input_PUI_ref) + 1)\n",
    "\n",
    "        '''\n",
    "        Create a column MutiComp_index to save the index of ICAM -> CEXX_1, CEXX_2 in per decomposition record.\n",
    "        The record can be traced via the original input PUI frame to get the original multi-chip record\n",
    "        '''\n",
    "\n",
    "        # Create new column, COMP_12NC by copy \"1_12NC_LIST\" if that is not multi-chip (ignore ICFT), and transder the series type to string\n",
    "        input_PUI_ref[\"SingleComp_index\"] = input_PUI_ref.index\n",
    "        input_PUI_ref[\"COMP_12NC\"] = input_PUI_ref[\"1_12NC_LIST\"].copy()\n",
    "        input_PUI_ref.loc[(input_PUI_ref[\"COMP_12NC_LIST\"].str.contains(\",\")) & (input_PUI_ref[\"TYPE\"] == \"ICAM\"), \"COMP_12NC\"] = input_PUI_ref.loc[(input_PUI_ref[\"COMP_12NC_LIST\"].str.contains(\",\")) & (input_PUI_ref[\"TYPE\"] == \"ICAM\"), \"COMP_12NC_LIST\"]\n",
    "        input_PUI_ref[\"COMP_12NC\"] = input_PUI_ref[\"COMP_12NC\"].astype(str)\n",
    "\n",
    "        # Get component 12NC and extend them if they are multi-chip to an independent dataframe\n",
    "        df_connects_multi_CEXX = input_PUI_ref.loc[input_PUI_ref[\"COMP_12NC\"].str.contains(\",\")]\n",
    "        df_connects_multi_CEXX.loc[df_connects_multi_CEXX[\"COMP_12NC\"] .str.contains(\",\"), \"MutiComp\"] = df_connects_multi_CEXX.loc[df_connects_multi_CEXX[\"COMP_12NC\"].str.contains(\",\"), \"COMP_12NC\"]\n",
    "\n",
    "        df_connects_multi_CEXX[\"SingleComp_index\"] = pd.NaT\n",
    "        df_connects_multi_CEXX.loc[df_connects_multi_CEXX[\"COMP_12NC\"].str.contains(\",\"), \"MutiComp_index\"] = df_connects_multi_CEXX.loc[df_connects_multi_CEXX[\"COMP_12NC\"].str.contains(\",\")].index\n",
    "        df_connects_multi_CEXX[\"MutiComp_index\"] = df_connects_multi_CEXX[\"MutiComp_index\"].astype(int)\n",
    "\n",
    "        def split_rows(df, column):\n",
    "            # Create an empty DataFrame to store the new rows\n",
    "            new_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                # Check if the value in the specific column contains a comma\n",
    "                if \",\" in str(row[column]):\n",
    "                    # Split the value by comma\n",
    "                    split_values = str(row[column]).split(',')\n",
    "                    for value in split_values:\n",
    "                        # Create a new row with the split value\n",
    "                        new_row = row.copy()\n",
    "                        new_row[column] = value.strip()  # Remove any leading/trailing whitespace\n",
    "                        new_df = new_df.append(new_row, ignore_index=True)\n",
    "                else:\n",
    "                    # If no comma, just append the row as is\n",
    "                    new_df = new_df.append(row, ignore_index=True)\n",
    "            \n",
    "            return new_df\n",
    "\n",
    "        df_extend_multicomp = split_rows(df_connects_multi_CEXX, 'COMP_12NC')\n",
    "        df_extend_multicomp[\"COMP_12NC\"] = df_extend_multicomp[\"COMP_12NC\"].astype('str')\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        df_PUI_raw = original PUI table\n",
    "        df_extend_multicomp = independent dataframe to save the extended CEXX connects to ICAM where ICAM is multichip (per ICAM -> CEXX per row)\n",
    "        df_agg = Aggregated Result of df_PUI_raw and df_agg and reset the index.\n",
    "        df_agg_final = drop (ICAM -> CEXX_1, CEXX_2) in df_agg to make the data available for rolling. SingleComp_index, MutiComp_index can be the tracing index from df_agg\n",
    "\n",
    "        '''\n",
    "\n",
    "        df_agg = pd.concat([input_PUI_ref, df_extend_multicomp])\n",
    "        df_agg.reset_index(drop = True, inplace =True)\n",
    "        df_agg.index = np.arange(1, len(df_agg) + 1)\n",
    "\n",
    "        print(\"\\nLength of the Original PUI table: {}\".format(len(input_PUI_ref)))\n",
    "        print(\"Length of the Extended Multi-Component PUI table: {}\".format(len(df_extend_multicomp)))\n",
    "        print(\"Length of the Aggregated PUI table: {}, {}\".format(len(df_agg), len(df_agg) == (len(input_PUI_ref) + len(df_extend_multicomp))))\n",
    "\n",
    "        df_agg_final = df_agg[~df_agg.index.isin(df_agg[\"MutiComp_index\"].dropna())]\n",
    "\n",
    "        print(\"Length of the Aggreaged PUI and drop multiple list: {}\\n\".format(len(df_agg_final)))\n",
    "\n",
    "        # Dict for adjacent 12NC\n",
    "        df_agg_final[\"PART_12NC\"] = df_agg_final[\"PART_12NC\"].astype(\"int64\")\n",
    "        df_agg_final[\"COMP_12NC\"] = df_agg_final[\"COMP_12NC\"].astype(\"int64\")\n",
    "\n",
    "        # print(\"\\nData Schema Aggregated final PUI result: \")\n",
    "        # print(df_agg_final.info())\n",
    "        return input_PUI_ref, df_agg, df_agg_final\n",
    "    \n",
    "\n",
    "    def Prod_Consumsed_Adjacency(self, input_df_agg_final):\n",
    "        # Dict for adjacent 12NC\n",
    "        prod = {}\n",
    "        for x in input_df_agg_final[\"PART_12NC\"].unique():\n",
    "            prod[x] = input_df_agg_final.loc[input_df_agg_final['PART_12NC'] == x, \"COMP_12NC\"].to_list()\n",
    "        return prod\n",
    "\n",
    "    def Mtype_Dict_Preprocessing(self, input_df_agg):\n",
    "        # Dict for Matertial Type\n",
    "        material_12nc_type_list = [[\"PART_12NC\", \"TYPE\"],\n",
    "                                [\"1_12NC_LIST\", \"1_CLASS\"], \n",
    "                                [\"2_12NC_LIST\", \"2_CLASS\"], \n",
    "                                [\"3_12NC_LIST\", \"3_CLASS\"], \n",
    "                                [\"4_12NC_LIST\", \"4_CLASS\"]]\n",
    "        def Merge(dict1, dict2):\n",
    "            res = {**dict1, **dict2}\n",
    "            return res\n",
    "\n",
    "        Mtype = dict()\n",
    "        for material_12nc_type in (material_12nc_type_list):\n",
    "            sub_material = input_df_agg[material_12nc_type[0]].dropna()\n",
    "            sub_type = input_df_agg[material_12nc_type[1]].dropna()\n",
    "            sub_Mtype = dict(zip(sub_material,sub_type))\n",
    "            Mtype = Merge(Mtype, sub_Mtype)\n",
    "        \n",
    "        return Mtype\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PUI_Structure_Rolling Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PUI_Structure_Rolling:\n",
    "    def __init__(self):\n",
    "        self.SC_Num = 0\n",
    "        self.PUI_anlyzied_slice_stack = pd.DataFrame([])\n",
    "        self.df_all_valid_supply_path = pd.DataFrame([])\n",
    "        self.PUI_stack = []\n",
    "\n",
    "    def main(self, input_SC_dataframe, input_PUI_dataframe, input_PUI_ref_before_drop_multi_comp):\n",
    "        self.input_SC_List_len = len(input_SC_dataframe)\n",
    "\n",
    "        print(\"Rolling Supply Path ===========================================================\")\n",
    "        for prod_item, consumed_item, consumed_plant, BOM_id in tqdm(zip(input_SC_dataframe[\"ITEM_NAME\"], input_SC_dataframe[\"CONSUMED_ITEM\"], input_SC_dataframe[\"PLANT\"], input_SC_dataframe[\"ITEM_BOM_RT_ID\"]), total = len(input_SC_dataframe)):\n",
    "            self.SC_Num += 1\n",
    "            all_paths_list = self.find_unique_path(prod, prod_item, consumed_item) # Get all applicable paths (prod to comp) -> list\n",
    "            if len(all_paths_list) == 0:\n",
    "                continue\n",
    "            self.df_all_valid_supply_path = pd.concat([self.df_all_valid_supply_path, self.Mtype_Aggregation(all_paths_list)]) # Get all applicable supply chain with Mtype Mapping -> dataframe\n",
    "            df_SC_analyezd_Stack = self.SC_Structure_Extraction(input_PUI_dataframe, all_paths_list, consumed_plant, BOM_id) # Extract each supply path and check if the supply path is valid\n",
    "            \n",
    "            #Result concating for each input supply path combination\n",
    "            df_SC_analyezd_Stack[\"SC Index\"] = self.SC_Num\n",
    "            self.PUI_anlyzied_slice_stack = pd.concat([self.PUI_anlyzied_slice_stack, df_SC_analyezd_Stack]) \n",
    "\n",
    "        if len(self.PUI_anlyzied_slice_stack) != 0:\n",
    "            self.PUI_anlyzied_slice_stack[\"Multi Chip\"] = self.PUI_anlyzied_slice_stack[\"ITEM_BOM_RT_ID\"].map(MultiChip_dict)\n",
    "            self.PUI_anlyzied_slice_stack[\"Produced_Item_Category\"] = self.PUI_anlyzied_slice_stack[\"Produced\"].map(Mtype)\n",
    "            self.PUI_anlyzied_slice_stack[\"Consumed_Item_Category\"] = self.PUI_anlyzied_slice_stack[\"Consumed\"].map(Mtype)\n",
    "            self.PUI_anlyzied_slice_stack.insert(0, 'SC Index', self.PUI_anlyzied_slice_stack.pop('SC Index'))\n",
    "            self.PUI_anlyzied_slice_stack.insert(2, 'ITEM_BOM_RT_ID', self.PUI_anlyzied_slice_stack.pop('ITEM_BOM_RT_ID'))\n",
    "            self.PUI_anlyzied_slice_stack.insert(3, \"Multi Chip\", self.PUI_anlyzied_slice_stack.pop(\"Multi Chip\"))\n",
    "            self.PUI_anlyzied_slice_stack.insert(5, \"Produced_Item_Category\", self.PUI_anlyzied_slice_stack.pop(\"Produced_Item_Category\"))\n",
    "            self.PUI_anlyzied_slice_stack.insert(7, \"Consumed_Item_Category\", self.PUI_anlyzied_slice_stack.pop(\"Consumed_Item_Category\"))\n",
    "\n",
    "        print(\"\\nMulti-Component Checking ======================================================\")\n",
    "        df_component_analyzed_result  = self.MultiComponent_Checking(self.PUI_anlyzied_slice_stack, input_SC_dataframe)\n",
    "\n",
    "        print(\"\\nPUI Mapping Result ============================================================\")\n",
    "        df_PUI_mapping = self.PUI_result_mapping(input_PUI_ref_before_drop_multi_comp, df_component_analyzed_result)\n",
    "        \n",
    "        #Result proprocessing\n",
    "        if len(df_PUI_mapping) != 0:\n",
    "            df_PUI_mapping[\"PUI_Index\"] = df_PUI_mapping.index\n",
    "            df_PUI_mapping[\"PART_12NC\"] = df_PUI_mapping[\"PART_12NC\"].astype(str)\n",
    "            df_PUI_mapping[\"COMP_12NC\"] = df_PUI_mapping[\"COMP_12NC\"].astype(str)\n",
    "            df_PUI_mapping.insert(0, 'PUI_Index', df_PUI_mapping.pop('PUI_Index'))\n",
    "            df_PUI_mapping.insert(1, 'SC Index', df_PUI_mapping.pop('SC Index'))\n",
    "            df_PUI_mapping.insert(2, 'ITEM_BOM_RT_ID', df_PUI_mapping.pop('ITEM_BOM_RT_ID'))\n",
    "            df_PUI_mapping.insert(10, 'COMP_12NC', df_PUI_mapping.pop('COMP_12NC'))\n",
    "            df_PUI_mapping.drop(columns = [\"SingleComp_index\", \"MutiComp\", \"MutiComp_index\"], inplace = True)\n",
    "            df_PUI_mapping.reset_index(drop= True, inplace = True)\n",
    "\n",
    "        #Mapping result for JDA input\n",
    "        input_SC_dataframe[\"Connected Rolling Structure\"] = \"No\"\n",
    "        input_SC_dataframe[\"Valid Rolling Structure\"] = \"No\"\n",
    "\n",
    "        if len(df_component_analyzed_result) != 0:\n",
    "            input_SC_dataframe.loc[input_SC_dataframe[\"ITEM_BOM_RT_ID\"].isin(df_component_analyzed_result[\"ITEM_BOM_RT_ID\"]), \"Connected Rolling Structure\"] = \"Yes\"\n",
    "            input_SC_dataframe.loc[input_SC_dataframe[\"ITEM_BOM_RT_ID\"].isin(df_component_analyzed_result.loc[df_component_analyzed_result[\"Valid_Comp_Combination\"] == True, \"ITEM_BOM_RT_ID\"].unique()), \"Valid Rolling Structure\"] = \"Yes\"\n",
    "\n",
    "        return df_PUI_mapping, df_component_analyzed_result, self.df_all_valid_supply_path, input_SC_dataframe\n",
    "\n",
    "    def find_all_paths(self, graph, start, end, path=None, unique_paths=None):\n",
    "        if path is None:\n",
    "            path = []\n",
    "        if unique_paths is None:\n",
    "            unique_paths = set()\n",
    "        path.append(start)\n",
    "        if start == end:\n",
    "            # Convert path to a tuple so it can be added to a set\n",
    "            unique_paths.add(tuple(path))\n",
    "        else:\n",
    "            for node in graph.get(start, []):\n",
    "                if node not in path:  # Avoid cycles\n",
    "                    self.find_all_paths(graph, node, end, path.copy(), unique_paths)\n",
    "        return unique_paths\n",
    "\n",
    "\n",
    "    def find_unique_path(self, graph, start, end, path=None, unique_paths=None):\n",
    "        # Find all paths\n",
    "        all_paths = self.find_all_paths(graph, start, end)\n",
    "        # Convert each tuple path back to a list if needed\n",
    "        all_paths = [list(path) for path in all_paths]\n",
    "        # print(\"============================================================\")\n",
    "        # print(\"Feasible Combinations: \")\n",
    "        # print(\"({}/{})\".format(self.SC_Num, self.input_SC_List_len) , all_paths)\n",
    "        return all_paths\n",
    "\n",
    "\n",
    "    def Mtype_Aggregation(self, input_all_paths):\n",
    "        Info = []\n",
    "        for x in input_all_paths:\n",
    "            Info_sub = []\n",
    "            for y in x:\n",
    "                Info_sub.append(Mtype[y])\n",
    "            Info.append(Info_sub)\n",
    "\n",
    "        #Output\n",
    "        stacker = []\n",
    "        for x, y in zip(Info, input_all_paths):\n",
    "            stacker.append(x)\n",
    "            stacker.append(y)\n",
    "        output_df = pd.DataFrame(stacker)\n",
    "        output_df.columns = [('Element_' + str(x + 1)) for x in range(len(output_df.columns))]\n",
    "        output_df[\"SC Index\"] = self.SC_Num\n",
    "        output_df.insert(0, 'SC Index', output_df.pop('SC Index'))\n",
    "        return output_df\n",
    "\n",
    "    def SC_Structure_Extraction(self, input_PUI_ref, PUI_valid_path_list, consumed_plant, BOM_id):\n",
    "        SC_Stack = []\n",
    "        idx = 0\n",
    "        \n",
    "        for x in PUI_valid_path_list:\n",
    "            idx +=1\n",
    "            for y in range(len(x)):\n",
    "                try:\n",
    "                    SC_Stack.append([idx, x[y], x[y+1], input_PUI_ref.loc[(input_PUI_ref[\"PART_12NC\"] == x[y]) & (input_PUI_ref[\"COMP_12NC\"] == x[y+1]), \"PLANT\"].unique()])\n",
    "                except:\n",
    "                    # SC_Stack.append([\"\", \"\", \"\", \"\"])\n",
    "                    continue\n",
    "\n",
    "            \n",
    "        df_SC_analyezd_Stack = pd.DataFrame(SC_Stack, columns = [\"Rolling Result - Combination\", \"Produced\", \"Consumed\", \"Site\"])\n",
    "        #Check if target plant is in Available Plant for each combination\n",
    "        df_SC_analyezd_Stack[\"Target Plant\"] = consumed_plant\n",
    "        df_SC_analyezd_Stack[\"ITEM_BOM_RT_ID\"] = BOM_id\n",
    "        df_SC_analyezd_Stack[\"Available Plant\"] = df_SC_analyezd_Stack.apply(lambda x: True if consumed_plant in x[\"Site\"] else False, axis = 1)\n",
    "\n",
    "\n",
    "        for c in df_SC_analyezd_Stack[\"Rolling Result - Combination\"].unique():\n",
    "            if c != '':\n",
    "                each_comb = df_SC_analyezd_Stack.loc[(df_SC_analyezd_Stack[\"Rolling Result - Combination\"] == c)]\n",
    "                if (each_comb[\"Available Plant\"] == False).any() == True:\n",
    "                    df_SC_analyezd_Stack.loc[(df_SC_analyezd_Stack[\"Rolling Result - Combination\"] == c), \"Valid_Plant\"] = False\n",
    "                else:\n",
    "                    df_SC_analyezd_Stack.loc[(df_SC_analyezd_Stack[\"Rolling Result - Combination\"] == c), \"Valid_Plant\"] = True\n",
    "\n",
    "        df_SC_analyezd_Stack.fillna(\"\", inplace = True)\n",
    "        return df_SC_analyezd_Stack\n",
    "\n",
    "    def MultiComponent_Checking(self, input_PUI_anlyzied_slice_stack, input_SC_dataframe):\n",
    "        if len(input_PUI_anlyzied_slice_stack) == 0:\n",
    "            return pd.DataFrame([])\n",
    "        else:\n",
    "            input_PUI_anlyzied_slice_stack[\"Valid_Comp_Combination\"] = False\n",
    "            # Initialize all records are invalid in the beginning \n",
    "            for bom_id in tqdm(input_PUI_anlyzied_slice_stack[\"ITEM_BOM_RT_ID\"].unique(), total = len(input_PUI_anlyzied_slice_stack[\"ITEM_BOM_RT_ID\"].unique())):\n",
    "                df_extract_from_bom_id = input_PUI_anlyzied_slice_stack.loc[(input_PUI_anlyzied_slice_stack[\"ITEM_BOM_RT_ID\"] == bom_id)]\n",
    "                if len(df_extract_from_bom_id[\"SC Index\"].unique()) != len(input_SC_dataframe.loc[input_SC_dataframe[\"ITEM_BOM_RT_ID\"] == bom_id]):\n",
    "                    input_PUI_anlyzied_slice_stack.loc[(input_PUI_anlyzied_slice_stack[\"ITEM_BOM_RT_ID\"] == bom_id), \"Valid_Comp_Combination\"] = False\n",
    "                    continue\n",
    "                else:\n",
    "                    # If there is \n",
    "                    if sum(df_extract_from_bom_id[\"Multi Chip\"] == \"No\") == 0:\n",
    "                        #Check if it marked as Mutichip but only one SC is input\n",
    "                        if len(input_SC_dataframe.loc[input_SC_dataframe[\"ITEM_BOM_RT_ID\"] == bom_id]) == 1:\n",
    "                            continue\n",
    "\n",
    "                        # Get count of ICAM\n",
    "                        ICAM_count_per_SC_index = df_extract_from_bom_id.loc[df_extract_from_bom_id[\"Produced_Item_Category\"] ==\"ICAM\", [\"SC Index\", \"Produced\"]].drop_duplicates().value_counts(\"Produced\")\n",
    "                        Valid_ICAM_list = ICAM_count_per_SC_index[ICAM_count_per_SC_index >= len(input_SC_dataframe.loc[input_SC_dataframe[\"ITEM_BOM_RT_ID\"] == bom_id])].keys().to_list()\n",
    "                        for valid_ICAM in Valid_ICAM_list:\n",
    "                            # Get all valid ICAM combination (SC_Index, Combination)\n",
    "                            valid_ICAM_comb = df_extract_from_bom_id.loc[df_extract_from_bom_id[\"Produced\"] == valid_ICAM, [\"SC Index\", \"Rolling Result - Combination\"]]\n",
    "                            valid_ICAM_comb_mapping_result = df_extract_from_bom_id.merge(valid_ICAM_comb, how='inner', on = [\"SC Index\", \"Rolling Result - Combination\"])\n",
    "                            Checking_list_per_SC_index = []\n",
    "                            \n",
    "                            # Check if per SC combination has its valid ICAM\n",
    "                            for val_plant_checking_per_SC_index in valid_ICAM_comb_mapping_result[\"SC Index\"].unique():\n",
    "                                if sum(valid_ICAM_comb_mapping_result.loc[(valid_ICAM_comb_mapping_result[\"SC Index\"] == val_plant_checking_per_SC_index), \"Valid_Plant\"] == True) != 0:\n",
    "                                    if val_plant_checking_per_SC_index not in Checking_list_per_SC_index:\n",
    "                                        Checking_list_per_SC_index.append(val_plant_checking_per_SC_index)\n",
    "                            \n",
    "                            if len(Checking_list_per_SC_index) == len(valid_ICAM_comb[\"SC Index\"].unique()):\n",
    "                                for sc_idx, roll_comb in valid_ICAM_comb.values:\n",
    "                                    if sum(valid_ICAM_comb_mapping_result.loc[(valid_ICAM_comb_mapping_result[\"SC Index\"] == sc_idx) & (valid_ICAM_comb_mapping_result[\"Rolling Result - Combination\"] == roll_comb), \"Valid_Plant\"] == False) == 0:\n",
    "                                        input_PUI_anlyzied_slice_stack.loc[(input_PUI_anlyzied_slice_stack[\"SC Index\"] == sc_idx) & (input_PUI_anlyzied_slice_stack[\"Rolling Result - Combination\"] == roll_comb), \"Valid_Comp_Combination\"] = True\n",
    "                    else:\n",
    "                        for comb_id in df_extract_from_bom_id[\"Rolling Result - Combination\"].unique():\n",
    "                            df_extract_from_bom_id_and_comb_id = df_extract_from_bom_id[df_extract_from_bom_id[\"Rolling Result - Combination\"] == comb_id]\n",
    "                            if (df_extract_from_bom_id_and_comb_id[\"Valid_Plant\"] == False).any():\n",
    "                                input_PUI_anlyzied_slice_stack.loc[(input_PUI_anlyzied_slice_stack[\"ITEM_BOM_RT_ID\"] == bom_id) & (input_PUI_anlyzied_slice_stack[\"Rolling Result - Combination\"] == comb_id), \"Valid_Comp_Combination\"] = False\n",
    "                            else:\n",
    "                                input_PUI_anlyzied_slice_stack.loc[(input_PUI_anlyzied_slice_stack[\"ITEM_BOM_RT_ID\"] == bom_id) & (input_PUI_anlyzied_slice_stack[\"Rolling Result - Combination\"] == comb_id), \"Valid_Comp_Combination\"] = True\n",
    "            return input_PUI_anlyzied_slice_stack\n",
    "\n",
    "    def PUI_result_mapping(self, input_PUI_ref_before_drop_multi_comp, input_SC_analyezd_Stack):\n",
    "        if len(input_SC_analyezd_Stack) ==0:\n",
    "            return pd.DataFrame([])\n",
    "        else:\n",
    "            for idx, bom_id, p, c, plt, val in tqdm(zip(input_SC_analyezd_Stack[\"SC Index\"], input_SC_analyezd_Stack[\"ITEM_BOM_RT_ID\"], input_SC_analyezd_Stack[\"Produced\"], input_SC_analyezd_Stack[\"Consumed\"], input_SC_analyezd_Stack[\"Target Plant\"], input_SC_analyezd_Stack[\"Valid_Comp_Combination\"]), total = len(input_SC_analyezd_Stack[\"SC Index\"])):\n",
    "                if val == True:\n",
    "                    sub_PUI_index_single = input_PUI_ref_before_drop_multi_comp.loc[(input_PUI_ref_before_drop_multi_comp[\"PART_12NC\"] == p) & (input_PUI_ref_before_drop_multi_comp[\"COMP_12NC\"] == str(c)) & (input_PUI_ref_before_drop_multi_comp[\"PLANT\"] == plt), \"SingleComp_index\"].dropna().values\n",
    "                    sub_PUI_index_multi = input_PUI_ref_before_drop_multi_comp.loc[(input_PUI_ref_before_drop_multi_comp[\"PART_12NC\"] == p) & (input_PUI_ref_before_drop_multi_comp[\"COMP_12NC\"] == str(c)) & (input_PUI_ref_before_drop_multi_comp[\"PLANT\"] == plt), \"MutiComp_index\"].dropna().values\n",
    "                    sub_PUI_index = set(sub_PUI_index_single).union(set(sub_PUI_index_multi))\n",
    "\n",
    "                    sub_PUI = input_PUI_ref_before_drop_multi_comp.loc[sub_PUI_index]\n",
    "                    sub_PUI[\"SC Index\"] = idx\n",
    "                    sub_PUI[\"ITEM_BOM_RT_ID\"] = bom_id\n",
    "                    \n",
    "                    ###\n",
    "                    # print(\"\\r({}/{})\".format(idx, input_SC_analyezd_Stack[\"SC Index\"].max), bom_id, p, c, plt, val, \"sub_PUI_index: {}\".format(sub_PUI_index), end = \"\", flush = True)\n",
    "\n",
    "                    if len(sub_PUI_index) != 0:\n",
    "                        #self.PUI_stack = pd.concat([self.PUI_stack, sub_PUI])\n",
    "                        self.PUI_stack.append(sub_PUI)\n",
    "\n",
    "            if len(self.PUI_stack) ==0:\n",
    "                return pd.DataFrame([])\n",
    "            else:\n",
    "                return pd.concat(self.PUI_stack)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Checking and Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuslt_PUI_anlyzied_slice.insert(0, 'SC Index', Reuslt_PUI_anlyzied_slice.pop('SC Index'))\n",
    "# Reuslt_PUI_anlyzied_slice\n",
    "\n",
    "# Result_SP.insert(0, 'SC Index', Result_SP.pop('SC Index'))\n",
    "# Result_SP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from styleframe import StyleFrame, Styler, utils\n",
    "\n",
    "class General_function:\n",
    "     def __init__(self):\n",
    "          self.today = (datetime.datetime.today()).strftime('%Y%m%d')\n",
    "          self.output_path = \"./Output/Result_table_{}.xlsx\".format(self.today)\n",
    "\n",
    "     def Export_to_excel_with_style(self, input_Result, input_Result_SP, input_Reuslt_PUI_anlyzied_slice, input_Result_JDA_source):\n",
    "          Result_frame = General_function().style_changes(input_Result, \n",
    "                                                          [[\"PUI_Index\", \"SC Index\", \"ITEM_BOM_RT_ID\"], [\"PART_12NC\", \"TYPE\", \"COMP_12NC\"], [\"COMP_12NC_LIST\"], [\"PLANT\"]],\n",
    "                                                          [\"yellow\", \"#85C1E9\", \"#85C1E9\", \"green\"],\n",
    "                                                          )\n",
    "          try:\n",
    "               input_Result_drop_dup = input_Result.drop(columns=[\"SC Index\", \"ITEM_BOM_RT_ID\"]).drop_duplicates().sort_values(by = \"PUI_Index\")\n",
    "          except:\n",
    "               input_Result_drop_dup = input_Result.copy()\n",
    "\n",
    "          Result_drop_dup_frame = General_function().style_changes(input_Result_drop_dup, \n",
    "                                                          [[\"PUI_Index\"], [\"PART_12NC\", \"TYPE\", \"COMP_12NC\"], [\"COMP_12NC_LIST\"], [\"PLANT\"]],\n",
    "                                                          [\"yellow\", \"#85C1E9\", \"#85C1E9\", \"green\"],\n",
    "                                                          )\n",
    "          Result_SP_frame = General_function().style_changes(input_Result_SP.reset_index(drop = True), \n",
    "                                                          [[\"SC Index\", \"Rolling Result - Combination\"], [\"Produced\", \"Produced_Item_Category\", \"Consumed\", \"Consumed_Item_Category\"], [\"Target Plant\"], [\"ITEM_BOM_RT_ID\"]],\n",
    "                                                          [\"yellow\", \"#85C1E9\", \"green\"],\n",
    "                                                          )\n",
    "\n",
    "          Reuslt_PUI_anlyzied_slice_frame = General_function().style_changes(input_Reuslt_PUI_anlyzied_slice.reset_index(drop = True),\n",
    "                                                          [[\"SC Index\"]],\n",
    "                                                          [\"yellow\"],\n",
    "                                                          )\n",
    "          input_Result_JDA_source_frame = General_function().style_changes(input_Result_JDA_source,\n",
    "                                                            [[\"SC Index\", \"Connected Rolling Structure\", \"Valid Rolling Structure\"], [\"ITEM_NAME\", \"ITEM_CATEGORY\", \"CONSUMED_ITEM\", \"CONSUMED_ITEM_CATEGORY\"], [\"PLANT\"]],\n",
    "                                                            [\"yellow\", \"#85C1E9\", \"green\"]\n",
    "                                                            )\n",
    "          #Export to excel\n",
    "          excel_writer = StyleFrame.ExcelWriter(self.output_path)\n",
    "          Result_frame.to_excel(excel_writer, sheet_name = \"Result - PUI Mapping\", index= False, header = True)\n",
    "          Result_drop_dup_frame.to_excel(excel_writer, sheet_name = \"Result - PUI Mapping (Drop_dup)\", index= False, header = True)\n",
    "          Result_SP_frame.to_excel(excel_writer, sheet_name = \"Analyzed - Rolling Supply Path\", index= False, header = True)\n",
    "          Reuslt_PUI_anlyzied_slice_frame.to_excel(excel_writer, sheet_name = \"Initial - Rolling Supply Chain\", index= False, header = True)\n",
    "          input_Result_JDA_source_frame.to_excel(excel_writer, sheet_name = \"Input - JDA Source\", index= False, header = True)\n",
    "          excel_writer.save()\n",
    "\n",
    "          return \"Data exported completedly !\"\n",
    "          \n",
    "     def style_changes(self, input_frame, update_columns_combination, color_combination):\n",
    "          #Summary Table\n",
    "          if len(input_frame) == 0:\n",
    "               return StyleFrame(pd.DataFrame([]))\n",
    "          else:\n",
    "               sf_output_ref = StyleFrame(input_frame)\n",
    "               for h_col in input_frame.columns:\n",
    "                    sf_output_ref[h_col] = input_frame[h_col].fillna(\"\").values\n",
    "                    if h_col not in [\"EFF_FR_DATE\", \"EFF_TO_DATE\"]:\n",
    "                         sf_output_ref[h_col] = sf_output_ref[h_col].astype(str)\n",
    "                    header_width = input_frame[h_col].astype(str).str.len().max() + 10\n",
    "                    if header_width <= len(h_col):\n",
    "                         header_width = len(h_col) + 7\n",
    "                    sf_output_ref.apply_column_style(cols_to_style = h_col,\n",
    "                                                  styler_obj=Styler(font=utils.fonts.calibri, font_size= 11),\n",
    "                                                  width=header_width,\n",
    "                                                  style_header=False\n",
    "                                                  )\n",
    "\n",
    "                    \n",
    "               sf_output_ref.apply_headers_style(styler_obj = Styler(font = 'Calibri', font_size = 11, bg_color = \"#FDEBD0\"), cols_to_style = input_frame.columns)\n",
    "               for col_list, color in zip(update_columns_combination, color_combination):\n",
    "                    sf_output_ref.apply_headers_style(styler_obj = Styler(font = 'Calibri', font_size = 11, bg_color = color), cols_to_style = col_list)\n",
    "               return sf_output_ref     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loading ....\n",
      "Finish data loading in 0.34 mins.\n",
      "==============================================================\n",
      "\n",
      "Length of the Original PUI table: 59929\n",
      "Length of the Extended Multi-Component PUI table: 1048\n",
      "Length of the Aggregated PUI table: 60977, True\n",
      "Length of the Aggreaged PUI and drop multiple list: 60462\n",
      "\n",
      "Finish data preprocessing in 0.29 mins. Begin with data rolling...\n",
      "Rolling Supply Path ===========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 272.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multi-Component Checking ======================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 333.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PUI Mapping Result ============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Result is now exporting to excel files ...... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Export Process is now completed in 0.0 mins!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__=='__main__':\n",
    "    prod, MultiChip_dict, df_JDA_input, df_PUI_raw, df_agg, df_agg_final, Mtype = Data_preprocessing().main()\n",
    "    Result, Result_SP, Reuslt_PUI_anlyzied_slice, Result_JDA_source = PUI_Structure_Rolling().main(df_JDA_input, df_agg_final, df_agg)\n",
    "\n",
    "    # Export to excel with style\n",
    "    print(\"\\n>> Result is now exporting to excel files ...... \")\n",
    "    Excel_Exportint_Start_Time = time.time()\n",
    "    General_function().Export_to_excel_with_style(Result, Result_SP, Reuslt_PUI_anlyzied_slice, Result_JDA_source)\n",
    "    print(\">> Export Process is now completed in {} mins!\".format(round((time.time()-Excel_Exportint_Start_Time)/60, 2)))\n",
    "\n",
    "    os.startfile(os.getcwd().replace(\"\\\\\", \"/\") + \"/Output/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Process End ! Time spent: 0.51 mins\n"
     ]
    }
   ],
   "source": [
    "print(\"Analysis Process End ! Time spent: {time_spent} mins\".format(time_spent = round((time.time()-Program_Start_Time)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PBI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
